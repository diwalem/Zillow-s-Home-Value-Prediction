{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Short-Term Memory Network \n",
    "\n",
    "The Long Short-Term Memory network, or LSTM network, is a recurrent neural network that is trained using Backpropagation Through Time and overcomes the vanishing gradient problem.LSTM networks have memory blocks that are connected through layers.\n",
    "\n",
    "Given the log error of this month, what is the log error (log(zestimate)- log(sales price)) next month?\n",
    "\n",
    "We can write a simple function to convert our single column of data into a two-column dataset: the first column containing this month’s (t) log Error count and the second column containing next month’s (t+1) log Error, to be predicted.\n",
    "\n",
    "Before we get started, let’s first import all of the functions and classes we intend to use. This assumes a working SciPy environment with the Keras deep learning library installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.layers import Dropout, BatchNormalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixing the random number seed to ensure our results are reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2728: DtypeWarning: Columns (22,32,34,49,55) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Label Encoder on properties\n",
      "Creating training set:\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"train_2016_v2.csv\", parse_dates=[\"transactiondate\"])\n",
    "prop = pd.read_csv('properties 2016.csv')\n",
    "sample = pd.read_csv('sample_submission.csv')\n",
    " \n",
    "print('Fitting Label Encoder on properties')\n",
    "for c in prop.columns:\n",
    "    prop[c]=prop[c].fillna(-1)\n",
    "    if prop[c].dtype == 'object':\n",
    "        lbl = LabelEncoder()\n",
    "        lbl.fit(list(prop[c].values))\n",
    "        prop[c] = lbl.transform(list(prop[c].values))\n",
    "        \n",
    "#Create df_train and x_train y_train from that\n",
    "print('Creating training set:')\n",
    "df_train = train.merge(prop, how='left', on='parcelid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTMs are sensitive to the scale of the input data, specifically when the sigmoid (default) or tanh activation functions are used. \n",
    "\n",
    "So, we rescale the data to the range of 0-to-1, also called normalizing.We can easily normalize the dataset using the MinMaxScaler preprocessing class from the scikit-learn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.fillna(-1.0)\n",
    "dataset = df_train[['logerror']]\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "dataset = scaler.fit_transform(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After modelling our data and estimate the skill of our model on the training dataset, we need to get an idea of the skill of the model on new unseen data. \n",
    "For a normal classification or regression problem, we would do this using cross validation.\n",
    "\n",
    "With time series data, the sequence of values is important. \n",
    "A simple method that we can use is to split the ordered dataset into train and test datasets. \n",
    "The code below calculates the index of the split point and separates the data into the training datasets with 90% of the \n",
    "observations that we can use to train our model, leaving the remaining 10% for testing the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81247 9028\n"
     ]
    }
   ],
   "source": [
    "# split into train and test sets\n",
    "train_size = int(len(dataset) * 0.90)\n",
    "test_size = len(dataset) - train_size\n",
    "train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "print(len(train), len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function to create new dataset;\n",
    "\n",
    "The function takes two arguments: the dataset, which is a NumPy array that we want to convert into a dataset, and the look_back, which is the number of previous time steps to use as input variables to predict the next time \n",
    "period -in this case defaulted to 1.\n",
    "\n",
    "This default will create a dataset where X is the log Error at a given time (t)\n",
    "and Y is the log Error at the next time (t + 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, look_back=1):\n",
    "    dataX, dataY = [],[]\n",
    "    for i in range(len(dataset)-look_back):\n",
    "        a = dataset[i:(i+look_back), :]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back, :])\n",
    "    return np.array(dataX), np.array(dataY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a function build_model which takes the parameters train,test, look_back, activation, optimizer, epoch and loss as arguments.\n",
    "\n",
    "The LSTM network expects the input data (X) to be provided with a specific array structure in the form of: [samples, time steps, features].\n",
    "\n",
    "Currently, our data is in the form: [samples, features] and we are framing the problem as one time step for each sample. We can transform the prepared train and test input data into the expected structure using numpy.reshape().\n",
    "\n",
    "To design and fit our LSTM network for this problem,\n",
    "The network has a visible layer with 1 input, a hidden layer with 4 LSTM blocks or neurons, and an output layer that makes a single value prediction. The default sigmoid activation function is used for the LSTM blocks. The network is trained for 100 epochs and a batch size of 1 is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "look_back = 1\n",
    "def build_model(train,test,look_back,activation,optimizer,epochs,loss):\n",
    "    # reshape into X=t and Y=t+1\n",
    "    trainX, trainY = create_dataset(train, look_back)\n",
    "    testX, testY = create_dataset(test, look_back)\n",
    "    # reshape input to be [samples, time steps, features]\n",
    "    trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "    testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
    "    # create and fit the LSTM network\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(4, input_shape=(1, look_back),activation = activation))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss=loss, optimizer=optimizer)\n",
    "    model.fit(trainX, trainY, epochs=epochs, batch_size=256, verbose=2)\n",
    "    # make predictions\n",
    "    trainPredict = model.predict(trainX)\n",
    "    testPredict = model.predict(testX)\n",
    "    # invert predictions\n",
    "    trainPredict = scaler.inverse_transform(trainPredict)\n",
    "    trainY = scaler.inverse_transform(trainY)\n",
    "    testPredict = scaler.inverse_transform(testPredict)\n",
    "    testY = scaler.inverse_transform(testY)\n",
    "    # calculate root mean squared error\n",
    "    trainScore = math.sqrt(mean_squared_error(trainY, trainPredict[:, 0]))\n",
    "    print('Train Score: %.2f RMSE' % (trainScore))\n",
    "    testScore = math.sqrt(mean_squared_error(testY, testPredict[:, 0]))\n",
    "    print('Test Score: %.2f RMSE' % (testScore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function build_model is called various times with changing the activation functions (sigmoid, relu, tanh), optimizers (adam, adagrad) and loss functions (mean_squared_error,hinge, logcosh) for Epoch=100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " - 2s - loss: 0.0771\n",
      "Epoch 2/100\n",
      " - 2s - loss: 4.8103e-04\n",
      "Epoch 3/100\n",
      " - 2s - loss: 3.0218e-04\n",
      "Epoch 4/100\n",
      " - 2s - loss: 3.0219e-04\n",
      "Epoch 5/100\n",
      " - 2s - loss: 3.0219e-04\n",
      "Epoch 6/100\n",
      " - 2s - loss: 3.0220e-04\n",
      "Epoch 7/100\n",
      " - 2s - loss: 3.0221e-04\n",
      "Epoch 8/100\n",
      " - 2s - loss: 3.0221e-04\n",
      "Epoch 9/100\n",
      " - 2s - loss: 3.0223e-04\n",
      "Epoch 10/100\n",
      " - 2s - loss: 3.0220e-04\n",
      "Epoch 11/100\n",
      " - 2s - loss: 3.0222e-04\n",
      "Epoch 12/100\n",
      " - 2s - loss: 3.0226e-04\n",
      "Epoch 13/100\n",
      " - 2s - loss: 3.0226e-04\n",
      "Epoch 14/100\n",
      " - 2s - loss: 3.0227e-04\n",
      "Epoch 15/100\n",
      " - 2s - loss: 3.0230e-04\n",
      "Epoch 16/100\n",
      " - 2s - loss: 3.0225e-04\n",
      "Epoch 17/100\n",
      " - 2s - loss: 3.0227e-04\n",
      "Epoch 18/100\n",
      " - 2s - loss: 3.0233e-04\n",
      "Epoch 19/100\n",
      " - 2s - loss: 3.0238e-04\n",
      "Epoch 20/100\n",
      " - 2s - loss: 3.0245e-04\n",
      "Epoch 21/100\n",
      " - 2s - loss: 3.0252e-04\n",
      "Epoch 22/100\n",
      " - 2s - loss: 3.0256e-04\n",
      "Epoch 23/100\n",
      " - 2s - loss: 3.0256e-04\n",
      "Epoch 24/100\n",
      " - 2s - loss: 3.0274e-04\n",
      "Epoch 25/100\n",
      " - 2s - loss: 3.0256e-04\n",
      "Epoch 26/100\n",
      " - 2s - loss: 3.0271e-04\n",
      "Epoch 27/100\n",
      " - 2s - loss: 3.0298e-04\n",
      "Epoch 28/100\n",
      " - 2s - loss: 3.0285e-04\n",
      "Epoch 29/100\n",
      " - 2s - loss: 3.0309e-04\n",
      "Epoch 30/100\n",
      " - 2s - loss: 3.0278e-04\n",
      "Epoch 31/100\n",
      " - 2s - loss: 3.0338e-04\n",
      "Epoch 32/100\n",
      " - 2s - loss: 3.0284e-04\n",
      "Epoch 33/100\n",
      " - 2s - loss: 3.0348e-04\n",
      "Epoch 34/100\n",
      " - 2s - loss: 3.0369e-04\n",
      "Epoch 35/100\n",
      " - 2s - loss: 3.0326e-04\n",
      "Epoch 36/100\n",
      " - 2s - loss: 3.0338e-04\n",
      "Epoch 37/100\n",
      " - 2s - loss: 3.0316e-04\n",
      "Epoch 38/100\n",
      " - 2s - loss: 3.0319e-04\n",
      "Epoch 39/100\n",
      " - 2s - loss: 3.0278e-04\n",
      "Epoch 40/100\n",
      " - 2s - loss: 3.0307e-04\n",
      "Epoch 41/100\n",
      " - 2s - loss: 3.0336e-04\n",
      "Epoch 42/100\n",
      " - 2s - loss: 3.0306e-04\n",
      "Epoch 43/100\n",
      " - 2s - loss: 3.0301e-04\n",
      "Epoch 44/100\n",
      " - 2s - loss: 3.0310e-04\n",
      "Epoch 45/100\n",
      " - 2s - loss: 3.0289e-04\n",
      "Epoch 46/100\n",
      " - 2s - loss: 3.0330e-04\n",
      "Epoch 47/100\n",
      " - 2s - loss: 3.0308e-04\n",
      "Epoch 48/100\n",
      " - 2s - loss: 3.0374e-04\n",
      "Epoch 49/100\n",
      " - 2s - loss: 3.0323e-04\n",
      "Epoch 50/100\n",
      " - 2s - loss: 3.0374e-04\n",
      "Epoch 51/100\n",
      " - 2s - loss: 3.0352e-04\n",
      "Epoch 52/100\n",
      " - 2s - loss: 3.0312e-04\n",
      "Epoch 53/100\n",
      " - 2s - loss: 3.0316e-04\n",
      "Epoch 54/100\n",
      " - 2s - loss: 3.0337e-04\n",
      "Epoch 55/100\n",
      " - 2s - loss: 3.0283e-04\n",
      "Epoch 56/100\n",
      " - 2s - loss: 3.0292e-04\n",
      "Epoch 57/100\n",
      " - 2s - loss: 3.0317e-04\n",
      "Epoch 58/100\n",
      " - 2s - loss: 3.0328e-04\n",
      "Epoch 59/100\n",
      " - 2s - loss: 3.0357e-04\n",
      "Epoch 60/100\n",
      " - 2s - loss: 3.0311e-04\n",
      "Epoch 61/100\n",
      " - 2s - loss: 3.0328e-04\n",
      "Epoch 62/100\n",
      " - 2s - loss: 3.0298e-04\n",
      "Epoch 63/100\n",
      " - 2s - loss: 3.0389e-04\n",
      "Epoch 64/100\n",
      " - 2s - loss: 3.0323e-04\n",
      "Epoch 65/100\n",
      " - 2s - loss: 3.0355e-04\n",
      "Epoch 66/100\n",
      " - 2s - loss: 3.0347e-04\n",
      "Epoch 67/100\n",
      " - 2s - loss: 3.0348e-04\n",
      "Epoch 68/100\n",
      " - 2s - loss: 3.0317e-04\n",
      "Epoch 69/100\n",
      " - 2s - loss: 3.0354e-04\n",
      "Epoch 70/100\n",
      " - 2s - loss: 3.0307e-04\n",
      "Epoch 71/100\n",
      " - 2s - loss: 3.0285e-04\n",
      "Epoch 72/100\n",
      " - 2s - loss: 3.0309e-04\n",
      "Epoch 73/100\n",
      " - 2s - loss: 3.0380e-04\n",
      "Epoch 74/100\n",
      " - 2s - loss: 3.0305e-04\n",
      "Epoch 75/100\n",
      " - 2s - loss: 3.0293e-04\n",
      "Epoch 76/100\n",
      " - 2s - loss: 3.0338e-04\n",
      "Epoch 77/100\n",
      " - 2s - loss: 3.0295e-04\n",
      "Epoch 78/100\n",
      " - 2s - loss: 3.0355e-04\n",
      "Epoch 79/100\n",
      " - 2s - loss: 3.0365e-04\n",
      "Epoch 80/100\n",
      " - 2s - loss: 3.0318e-04\n",
      "Epoch 81/100\n",
      " - 2s - loss: 3.0309e-04\n",
      "Epoch 82/100\n",
      " - 2s - loss: 3.0305e-04\n",
      "Epoch 83/100\n",
      " - 2s - loss: 3.0302e-04\n",
      "Epoch 84/100\n",
      " - 2s - loss: 3.0346e-04\n",
      "Epoch 85/100\n",
      " - 2s - loss: 3.0330e-04\n",
      "Epoch 86/100\n",
      " - 2s - loss: 3.0335e-04\n",
      "Epoch 87/100\n",
      " - 2s - loss: 3.0342e-04\n",
      "Epoch 88/100\n",
      " - 2s - loss: 3.0334e-04\n",
      "Epoch 89/100\n",
      " - 2s - loss: 3.0324e-04\n",
      "Epoch 90/100\n",
      " - 2s - loss: 3.0304e-04\n",
      "Epoch 91/100\n",
      " - 2s - loss: 3.0334e-04\n",
      "Epoch 92/100\n",
      " - 2s - loss: 3.0269e-04\n",
      "Epoch 93/100\n",
      " - 2s - loss: 3.0335e-04\n",
      "Epoch 94/100\n",
      " - 2s - loss: 3.0329e-04\n",
      "Epoch 95/100\n",
      " - 2s - loss: 3.0309e-04\n",
      "Epoch 96/100\n",
      " - 2s - loss: 3.0316e-04\n",
      "Epoch 97/100\n",
      " - 2s - loss: 3.0312e-04\n",
      "Epoch 98/100\n",
      " - 2s - loss: 3.0319e-04\n",
      "Epoch 99/100\n",
      " - 2s - loss: 3.0310e-04\n",
      "Epoch 100/100\n",
      " - 2s - loss: 3.0403e-04\n",
      "Train Score: 0.16 RMSE\n",
      "Test Score: 0.15 RMSE\n"
     ]
    }
   ],
   "source": [
    "build_model(train,test,1,'sigmoid','adam',100,'mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the model has an average error of about 0.16 on the training dataset, and about 0.15 on the test dataset. Of all the various functions used to predict and calculate the RMSE these set of activation, loss and optimizer functions give more accurate predictions of Log Error of home values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " - 2s - loss: 0.0323\n",
      "Epoch 2/100\n",
      " - 2s - loss: 3.7371e-04\n",
      "Epoch 3/100\n",
      " - 2s - loss: 3.7311e-04\n",
      "Epoch 4/100\n",
      " - 2s - loss: 3.7236e-04\n",
      "Epoch 5/100\n",
      " - 2s - loss: 3.7132e-04\n",
      "Epoch 6/100\n",
      " - 2s - loss: 3.7015e-04\n",
      "Epoch 7/100\n",
      " - 2s - loss: 3.6867e-04\n",
      "Epoch 8/100\n",
      " - 2s - loss: 3.6712e-04\n",
      "Epoch 9/100\n",
      " - 2s - loss: 3.6521e-04\n",
      "Epoch 10/100\n",
      " - 2s - loss: 3.6311e-04\n",
      "Epoch 11/100\n",
      " - 2s - loss: 3.6065e-04\n",
      "Epoch 12/100\n",
      " - 2s - loss: 3.5804e-04\n",
      "Epoch 13/100\n",
      " - 2s - loss: 3.5509e-04\n",
      "Epoch 14/100\n",
      " - 2s - loss: 3.5190e-04\n",
      "Epoch 15/100\n",
      " - 1s - loss: 3.4853e-04\n",
      "Epoch 16/100\n",
      " - 2s - loss: 3.4498e-04\n",
      "Epoch 17/100\n",
      " - 2s - loss: 3.4126e-04\n",
      "Epoch 18/100\n",
      " - 2s - loss: 3.3755e-04\n",
      "Epoch 19/100\n",
      " - 2s - loss: 3.3400e-04\n",
      "Epoch 20/100\n",
      " - 2s - loss: 3.2911e-04\n",
      "Epoch 21/100\n",
      " - 2s - loss: 3.2592e-04\n",
      "Epoch 22/100\n",
      " - 2s - loss: 3.2150e-04\n",
      "Epoch 23/100\n",
      " - 2s - loss: 3.1812e-04\n",
      "Epoch 24/100\n",
      " - 2s - loss: 3.1466e-04\n",
      "Epoch 25/100\n",
      " - 2s - loss: 3.1175e-04\n",
      "Epoch 26/100\n",
      " - 2s - loss: 3.0994e-04\n",
      "Epoch 27/100\n",
      " - 2s - loss: 3.0749e-04\n",
      "Epoch 28/100\n",
      " - 2s - loss: 3.0637e-04\n",
      "Epoch 29/100\n",
      " - 2s - loss: 3.0527e-04\n",
      "Epoch 30/100\n",
      " - 2s - loss: 3.0494e-04\n",
      "Epoch 31/100\n",
      " - 2s - loss: 3.0403e-04\n",
      "Epoch 32/100\n",
      " - 2s - loss: 3.0417e-04\n",
      "Epoch 33/100\n",
      " - 2s - loss: 3.0391e-04\n",
      "Epoch 34/100\n",
      " - 2s - loss: 3.0364e-04\n",
      "Epoch 35/100\n",
      " - 2s - loss: 3.0470e-04\n",
      "Epoch 36/100\n",
      " - 2s - loss: 3.0394e-04\n",
      "Epoch 37/100\n",
      " - 2s - loss: 3.0324e-04\n",
      "Epoch 38/100\n",
      " - 2s - loss: 3.0432e-04\n",
      "Epoch 39/100\n",
      " - 2s - loss: 3.0342e-04\n",
      "Epoch 40/100\n",
      " - 2s - loss: 3.0370e-04\n",
      "Epoch 41/100\n",
      " - 2s - loss: 3.0348e-04\n",
      "Epoch 42/100\n",
      " - 2s - loss: 3.0337e-04\n",
      "Epoch 43/100\n",
      " - 2s - loss: 3.0345e-04\n",
      "Epoch 44/100\n",
      " - 2s - loss: 3.0376e-04\n",
      "Epoch 45/100\n",
      " - 2s - loss: 3.0399e-04\n",
      "Epoch 46/100\n",
      " - 2s - loss: 3.0354e-04\n",
      "Epoch 47/100\n",
      " - 2s - loss: 3.0376e-04\n",
      "Epoch 48/100\n",
      " - 2s - loss: 3.0304e-04\n",
      "Epoch 49/100\n",
      " - 2s - loss: 3.0420e-04\n",
      "Epoch 50/100\n",
      " - 2s - loss: 3.0338e-04\n",
      "Epoch 51/100\n",
      " - 2s - loss: 3.0354e-04\n",
      "Epoch 52/100\n",
      " - 2s - loss: 3.0326e-04\n",
      "Epoch 53/100\n",
      " - 2s - loss: 3.0359e-04\n",
      "Epoch 54/100\n",
      " - 2s - loss: 3.0356e-04\n",
      "Epoch 55/100\n",
      " - 2s - loss: 3.0373e-04\n",
      "Epoch 56/100\n",
      " - 2s - loss: 3.0311e-04\n",
      "Epoch 57/100\n",
      " - 2s - loss: 3.0391e-04\n",
      "Epoch 58/100\n",
      " - 2s - loss: 3.0382e-04\n",
      "Epoch 59/100\n",
      " - 1s - loss: 3.0327e-04\n",
      "Epoch 60/100\n",
      " - 2s - loss: 3.0346e-04\n",
      "Epoch 61/100\n",
      " - 2s - loss: 3.0305e-04\n",
      "Epoch 62/100\n",
      " - 2s - loss: 3.0353e-04\n",
      "Epoch 63/100\n",
      " - 2s - loss: 3.0371e-04\n",
      "Epoch 64/100\n",
      " - 2s - loss: 3.0333e-04\n",
      "Epoch 65/100\n",
      " - 2s - loss: 3.0347e-04\n",
      "Epoch 66/100\n",
      " - 2s - loss: 3.0378e-04\n",
      "Epoch 67/100\n",
      " - 2s - loss: 3.0469e-04\n",
      "Epoch 68/100\n",
      " - 2s - loss: 3.0289e-04\n",
      "Epoch 69/100\n",
      " - 2s - loss: 3.0346e-04\n",
      "Epoch 70/100\n",
      " - 2s - loss: 3.0346e-04\n",
      "Epoch 71/100\n",
      " - 2s - loss: 3.0374e-04\n",
      "Epoch 72/100\n",
      " - 2s - loss: 3.0357e-04\n",
      "Epoch 73/100\n",
      " - 2s - loss: 3.0371e-04\n",
      "Epoch 74/100\n",
      " - 2s - loss: 3.0344e-04\n",
      "Epoch 75/100\n",
      " - 2s - loss: 3.0331e-04\n",
      "Epoch 76/100\n",
      " - 2s - loss: 3.0332e-04\n",
      "Epoch 77/100\n",
      " - 2s - loss: 3.0362e-04\n",
      "Epoch 78/100\n",
      " - 2s - loss: 3.0378e-04\n",
      "Epoch 79/100\n",
      " - 2s - loss: 3.0430e-04\n",
      "Epoch 80/100\n",
      " - 2s - loss: 3.0424e-04\n",
      "Epoch 81/100\n",
      " - 2s - loss: 3.0318e-04\n",
      "Epoch 82/100\n",
      " - 2s - loss: 3.0389e-04\n",
      "Epoch 83/100\n",
      " - 2s - loss: 3.0398e-04\n",
      "Epoch 84/100\n",
      " - 2s - loss: 3.0316e-04\n",
      "Epoch 85/100\n",
      " - 2s - loss: 3.0353e-04\n",
      "Epoch 86/100\n",
      " - 2s - loss: 3.0365e-04\n",
      "Epoch 87/100\n",
      " - 2s - loss: 3.0371e-04\n",
      "Epoch 88/100\n",
      " - 2s - loss: 3.0318e-04\n",
      "Epoch 89/100\n",
      " - 2s - loss: 3.0309e-04\n",
      "Epoch 90/100\n",
      " - 2s - loss: 3.0324e-04\n",
      "Epoch 91/100\n",
      " - 2s - loss: 3.0337e-04\n",
      "Epoch 92/100\n",
      " - 2s - loss: 3.0348e-04\n",
      "Epoch 93/100\n",
      " - 2s - loss: 3.0341e-04\n",
      "Epoch 94/100\n",
      " - 2s - loss: 3.0401e-04\n",
      "Epoch 95/100\n",
      " - 2s - loss: 3.0422e-04\n",
      "Epoch 96/100\n",
      " - 2s - loss: 3.0320e-04\n",
      "Epoch 97/100\n",
      " - 2s - loss: 3.0351e-04\n",
      "Epoch 98/100\n",
      " - 2s - loss: 3.0367e-04\n",
      "Epoch 99/100\n",
      " - 2s - loss: 3.0335e-04\n",
      "Epoch 100/100\n",
      " - 2s - loss: 3.0426e-04\n",
      "Train Score: 0.16 RMSE\n",
      "Test Score: 0.15 RMSE\n"
     ]
    }
   ],
   "source": [
    "build_model(train,test,1,'relu','adam',100,'mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the model has an average error of about 0.16 on the training dataset, and about 0.15 on the test dataset. Of all the various functions used to predict and calculate the RMSE these set of activation, loss and optimizer functions give more accurate predictions of Log Error of home values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " - 2s - loss: 0.0680\n",
      "Epoch 2/100\n",
      " - 2s - loss: 3.3425e-04\n",
      "Epoch 3/100\n",
      " - 2s - loss: 3.1591e-04\n",
      "Epoch 4/100\n",
      " - 2s - loss: 3.1582e-04\n",
      "Epoch 5/100\n",
      " - 2s - loss: 3.1572e-04\n",
      "Epoch 6/100\n",
      " - 2s - loss: 3.1558e-04\n",
      "Epoch 7/100\n",
      " - 2s - loss: 3.1542e-04\n",
      "Epoch 8/100\n",
      " - 2s - loss: 3.1522e-04\n",
      "Epoch 9/100\n",
      " - 2s - loss: 3.1505e-04\n",
      "Epoch 10/100\n",
      " - 2s - loss: 3.1480e-04\n",
      "Epoch 11/100\n",
      " - 2s - loss: 3.1452e-04\n",
      "Epoch 12/100\n",
      " - 2s - loss: 3.1418e-04\n",
      "Epoch 13/100\n",
      " - 2s - loss: 3.1380e-04\n",
      "Epoch 14/100\n",
      " - 2s - loss: 3.1345e-04\n",
      "Epoch 15/100\n",
      " - 2s - loss: 3.1283e-04\n",
      "Epoch 16/100\n",
      " - 2s - loss: 3.1251e-04\n",
      "Epoch 17/100\n",
      " - 2s - loss: 3.1188e-04\n",
      "Epoch 18/100\n",
      " - 2s - loss: 3.1128e-04\n",
      "Epoch 19/100\n",
      " - 2s - loss: 3.1059e-04\n",
      "Epoch 20/100\n",
      " - 2s - loss: 3.0999e-04\n",
      "Epoch 21/100\n",
      " - 2s - loss: 3.0934e-04\n",
      "Epoch 22/100\n",
      " - 2s - loss: 3.0865e-04\n",
      "Epoch 23/100\n",
      " - 2s - loss: 3.0793e-04\n",
      "Epoch 24/100\n",
      " - 2s - loss: 3.0741e-04\n",
      "Epoch 25/100\n",
      " - 2s - loss: 3.0632e-04\n",
      "Epoch 26/100\n",
      " - 2s - loss: 3.0572e-04\n",
      "Epoch 27/100\n",
      " - 2s - loss: 3.0539e-04\n",
      "Epoch 28/100\n",
      " - 2s - loss: 3.0473e-04\n",
      "Epoch 29/100\n",
      " - 2s - loss: 3.0461e-04\n",
      "Epoch 30/100\n",
      " - 2s - loss: 3.0484e-04\n",
      "Epoch 31/100\n",
      " - 2s - loss: 3.0415e-04\n",
      "Epoch 32/100\n",
      " - 2s - loss: 3.0370e-04\n",
      "Epoch 33/100\n",
      " - 2s - loss: 3.0390e-04\n",
      "Epoch 34/100\n",
      " - 2s - loss: 3.0353e-04\n",
      "Epoch 35/100\n",
      " - 2s - loss: 3.0391e-04\n",
      "Epoch 36/100\n",
      " - 2s - loss: 3.0379e-04\n",
      "Epoch 37/100\n",
      " - 2s - loss: 3.0398e-04\n",
      "Epoch 38/100\n",
      " - 2s - loss: 3.0297e-04\n",
      "Epoch 39/100\n",
      " - 2s - loss: 3.0359e-04\n",
      "Epoch 40/100\n",
      " - 2s - loss: 3.0368e-04\n",
      "Epoch 41/100\n",
      " - 2s - loss: 3.0345e-04\n",
      "Epoch 42/100\n",
      " - 2s - loss: 3.0331e-04\n",
      "Epoch 43/100\n",
      " - 2s - loss: 3.0323e-04\n",
      "Epoch 44/100\n",
      " - 2s - loss: 3.0337e-04\n",
      "Epoch 45/100\n",
      " - 2s - loss: 3.0368e-04\n",
      "Epoch 46/100\n",
      " - 2s - loss: 3.0357e-04\n",
      "Epoch 47/100\n",
      " - 2s - loss: 3.0373e-04\n",
      "Epoch 48/100\n",
      " - 2s - loss: 3.0345e-04\n",
      "Epoch 49/100\n",
      " - 2s - loss: 3.0327e-04\n",
      "Epoch 50/100\n",
      " - 2s - loss: 3.0333e-04\n",
      "Epoch 51/100\n",
      " - 2s - loss: 3.0347e-04\n",
      "Epoch 52/100\n",
      " - 2s - loss: 3.0373e-04\n",
      "Epoch 53/100\n",
      " - 2s - loss: 3.0392e-04\n",
      "Epoch 54/100\n",
      " - 2s - loss: 3.0319e-04\n",
      "Epoch 55/100\n",
      " - 2s - loss: 3.0326e-04\n",
      "Epoch 56/100\n",
      " - 2s - loss: 3.0311e-04\n",
      "Epoch 57/100\n",
      " - 2s - loss: 3.0349e-04\n",
      "Epoch 58/100\n",
      " - 2s - loss: 3.0322e-04\n",
      "Epoch 59/100\n",
      " - 2s - loss: 3.0344e-04\n",
      "Epoch 60/100\n",
      " - 2s - loss: 3.0319e-04\n",
      "Epoch 61/100\n",
      " - 2s - loss: 3.0320e-04\n",
      "Epoch 62/100\n",
      " - 2s - loss: 3.0344e-04\n",
      "Epoch 63/100\n",
      " - 2s - loss: 3.0377e-04\n",
      "Epoch 64/100\n",
      " - 2s - loss: 3.0344e-04\n",
      "Epoch 65/100\n",
      " - 2s - loss: 3.0326e-04\n",
      "Epoch 66/100\n",
      " - 2s - loss: 3.0299e-04\n",
      "Epoch 67/100\n",
      " - 2s - loss: 3.0301e-04\n",
      "Epoch 68/100\n",
      " - 2s - loss: 3.0345e-04\n",
      "Epoch 69/100\n",
      " - 2s - loss: 3.0330e-04\n",
      "Epoch 70/100\n",
      " - 2s - loss: 3.0343e-04\n",
      "Epoch 71/100\n",
      " - 2s - loss: 3.0331e-04\n",
      "Epoch 72/100\n",
      " - 2s - loss: 3.0355e-04\n",
      "Epoch 73/100\n",
      " - 2s - loss: 3.0311e-04\n",
      "Epoch 74/100\n",
      " - 2s - loss: 3.0341e-04\n",
      "Epoch 75/100\n",
      " - 2s - loss: 3.0314e-04\n",
      "Epoch 76/100\n",
      " - 2s - loss: 3.0315e-04\n",
      "Epoch 77/100\n",
      " - 2s - loss: 3.0325e-04\n",
      "Epoch 78/100\n",
      " - 2s - loss: 3.0307e-04\n",
      "Epoch 79/100\n",
      " - 2s - loss: 3.0358e-04\n",
      "Epoch 80/100\n",
      " - 2s - loss: 3.0389e-04\n",
      "Epoch 81/100\n",
      " - 2s - loss: 3.0341e-04\n",
      "Epoch 82/100\n",
      " - 2s - loss: 3.0359e-04\n",
      "Epoch 83/100\n",
      " - 2s - loss: 3.0329e-04\n",
      "Epoch 84/100\n",
      " - 2s - loss: 3.0292e-04\n",
      "Epoch 85/100\n",
      " - 2s - loss: 3.0367e-04\n",
      "Epoch 86/100\n",
      " - 2s - loss: 3.0309e-04\n",
      "Epoch 87/100\n",
      " - 2s - loss: 3.0333e-04\n",
      "Epoch 88/100\n",
      " - 2s - loss: 3.0332e-04\n",
      "Epoch 89/100\n",
      " - 2s - loss: 3.0317e-04\n",
      "Epoch 90/100\n",
      " - 2s - loss: 3.0361e-04\n",
      "Epoch 91/100\n",
      " - 2s - loss: 3.0329e-04\n",
      "Epoch 92/100\n",
      " - 2s - loss: 3.0353e-04\n",
      "Epoch 93/100\n",
      " - 2s - loss: 3.0279e-04\n",
      "Epoch 94/100\n",
      " - 2s - loss: 3.0287e-04\n",
      "Epoch 95/100\n",
      " - 2s - loss: 3.0351e-04\n",
      "Epoch 96/100\n",
      " - 2s - loss: 3.0342e-04\n",
      "Epoch 97/100\n",
      " - 2s - loss: 3.0274e-04\n",
      "Epoch 98/100\n",
      " - 2s - loss: 3.0307e-04\n",
      "Epoch 99/100\n",
      " - 2s - loss: 3.0345e-04\n",
      "Epoch 100/100\n",
      " - 2s - loss: 3.0317e-04\n",
      "Train Score: 0.16 RMSE\n",
      "Test Score: 0.15 RMSE\n"
     ]
    }
   ],
   "source": [
    "build_model(train,test,1,'tanh','adam',100,'mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has an average error of about 0.16 on the training dataset, and about 0.15 on the test dataset. Of all the various functions used to predict and calculate the RMSE these set of activation and optimizer functions give more accurate predictions of Log Error of home values with the loss function being MEAN_SQUARED_ERROR which measures the average of the squares of the errors or deviations —that is, the difference between the estimator and what is estimated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " - 2s - loss: 0.8655\n",
      "Epoch 2/100\n",
      " - 2s - loss: 0.2673\n",
      "Epoch 3/100\n",
      " - 2s - loss: 0.0011\n",
      "Epoch 4/100\n",
      " - 2s - loss: 8.3260e-04\n",
      "Epoch 5/100\n",
      " - 2s - loss: 7.0076e-04\n",
      "Epoch 6/100\n",
      " - 2s - loss: 6.1188e-04\n",
      "Epoch 7/100\n",
      " - 2s - loss: 5.4652e-04\n",
      "Epoch 8/100\n",
      " - 2s - loss: 4.9493e-04\n",
      "Epoch 9/100\n",
      " - 2s - loss: 4.5169e-04\n",
      "Epoch 10/100\n",
      " - 2s - loss: 4.1609e-04\n",
      "Epoch 11/100\n",
      " - 2s - loss: 3.8580e-04\n",
      "Epoch 12/100\n",
      " - 2s - loss: 3.5984e-04\n",
      "Epoch 13/100\n",
      " - 2s - loss: 3.3609e-04\n",
      "Epoch 14/100\n",
      " - 2s - loss: 3.1593e-04\n",
      "Epoch 15/100\n",
      " - 2s - loss: 2.9944e-04\n",
      "Epoch 16/100\n",
      " - 2s - loss: 2.8545e-04\n",
      "Epoch 17/100\n",
      " - 2s - loss: 2.7224e-04\n",
      "Epoch 18/100\n",
      " - 2s - loss: 2.5860e-04\n",
      "Epoch 19/100\n",
      " - 2s - loss: 2.4523e-04\n",
      "Epoch 20/100\n",
      " - 2s - loss: 2.3238e-04\n",
      "Epoch 21/100\n",
      " - 2s - loss: 2.2013e-04\n",
      "Epoch 22/100\n",
      " - 2s - loss: 2.0834e-04\n",
      "Epoch 23/100\n",
      " - 2s - loss: 1.9641e-04\n",
      "Epoch 24/100\n",
      " - 2s - loss: 1.8467e-04\n",
      "Epoch 25/100\n",
      " - 2s - loss: 1.7258e-04\n",
      "Epoch 26/100\n",
      " - 2s - loss: 1.6038e-04\n",
      "Epoch 27/100\n",
      " - 2s - loss: 1.4648e-04\n",
      "Epoch 28/100\n",
      " - 2s - loss: 1.3193e-04\n",
      "Epoch 29/100\n",
      " - 2s - loss: 1.1607e-04\n",
      "Epoch 30/100\n",
      " - 2s - loss: 9.7852e-05\n",
      "Epoch 31/100\n",
      " - 2s - loss: 7.6201e-05\n",
      "Epoch 32/100\n",
      " - 2s - loss: 5.2171e-05\n",
      "Epoch 33/100\n",
      " - 2s - loss: 3.7643e-05\n",
      "Epoch 34/100\n",
      " - 2s - loss: 3.4738e-05\n",
      "Epoch 35/100\n",
      " - 2s - loss: 3.4205e-05\n",
      "Epoch 36/100\n",
      " - 2s - loss: 3.3901e-05\n",
      "Epoch 37/100\n",
      " - 2s - loss: 3.3753e-05\n",
      "Epoch 38/100\n",
      " - 2s - loss: 3.3571e-05\n",
      "Epoch 39/100\n",
      " - 2s - loss: 3.3358e-05\n",
      "Epoch 40/100\n",
      " - 2s - loss: 3.3104e-05\n",
      "Epoch 41/100\n",
      " - 2s - loss: 3.2808e-05\n",
      "Epoch 42/100\n",
      " - 2s - loss: 3.2459e-05\n",
      "Epoch 43/100\n",
      " - 2s - loss: 3.2062e-05\n",
      "Epoch 44/100\n",
      " - 2s - loss: 3.1608e-05\n",
      "Epoch 45/100\n",
      " - 2s - loss: 3.1068e-05\n",
      "Epoch 46/100\n",
      " - 2s - loss: 3.0465e-05\n",
      "Epoch 47/100\n",
      " - 2s - loss: 2.9768e-05\n",
      "Epoch 48/100\n",
      " - 2s - loss: 2.9322e-05\n",
      "Epoch 49/100\n",
      " - 2s - loss: 2.8992e-05\n",
      "Epoch 50/100\n",
      " - 2s - loss: 2.8631e-05\n",
      "Epoch 51/100\n",
      " - 2s - loss: 2.8236e-05\n",
      "Epoch 52/100\n",
      " - 2s - loss: 2.7800e-05\n",
      "Epoch 53/100\n",
      " - 2s - loss: 2.7352e-05\n",
      "Epoch 54/100\n",
      " - 2s - loss: 2.6876e-05\n",
      "Epoch 55/100\n",
      " - 2s - loss: 2.6374e-05\n",
      "Epoch 56/100\n",
      " - 2s - loss: 2.6097e-05\n",
      "Epoch 57/100\n",
      " - 2s - loss: 2.5968e-05\n",
      "Epoch 58/100\n",
      " - 2s - loss: 2.5827e-05\n",
      "Epoch 59/100\n",
      " - 2s - loss: 2.5672e-05\n",
      "Epoch 60/100\n",
      " - 2s - loss: 2.5512e-05\n",
      "Epoch 61/100\n",
      " - 2s - loss: 2.5345e-05\n",
      "Epoch 62/100\n",
      " - 2s - loss: 2.5181e-05\n",
      "Epoch 63/100\n",
      " - 2s - loss: 2.5009e-05\n",
      "Epoch 64/100\n",
      " - 2s - loss: 2.4837e-05\n",
      "Epoch 65/100\n",
      " - 2s - loss: 2.4669e-05\n",
      "Epoch 66/100\n",
      " - 2s - loss: 2.4617e-05\n",
      "Epoch 67/100\n",
      " - 2s - loss: 2.4617e-05\n",
      "Epoch 68/100\n",
      " - 2s - loss: 2.4617e-05\n",
      "Epoch 69/100\n",
      " - 2s - loss: 2.4617e-05\n",
      "Epoch 70/100\n",
      " - 2s - loss: 2.4617e-05\n",
      "Epoch 71/100\n",
      " - 2s - loss: 2.4617e-05\n",
      "Epoch 72/100\n",
      " - 2s - loss: 2.4617e-05\n",
      "Epoch 73/100\n",
      " - 2s - loss: 2.4617e-05\n",
      "Epoch 74/100\n",
      " - 2s - loss: 2.4617e-05\n",
      "Epoch 75/100\n",
      " - 2s - loss: 2.4617e-05\n",
      "Epoch 76/100\n",
      " - 2s - loss: 2.4617e-05\n",
      "Epoch 77/100\n",
      " - 2s - loss: 2.4617e-05\n",
      "Epoch 78/100\n",
      " - 2s - loss: 2.4617e-05\n",
      "Epoch 79/100\n",
      " - 2s - loss: 2.4617e-05\n",
      "Epoch 80/100\n",
      " - 2s - loss: 2.4617e-05\n",
      "Epoch 81/100\n",
      " - 2s - loss: 2.4617e-05\n",
      "Epoch 82/100\n",
      " - 2s - loss: 2.4617e-05\n",
      "Epoch 83/100\n",
      " - 2s - loss: 2.4617e-05\n",
      "Epoch 84/100\n",
      " - 2s - loss: 2.4617e-05\n",
      "Epoch 85/100\n",
      " - 2s - loss: 2.4617e-05\n",
      "Epoch 86/100\n",
      " - 2s - loss: 2.4617e-05\n",
      "Epoch 87/100\n",
      " - 2s - loss: 2.4617e-05\n",
      "Epoch 88/100\n",
      " - 2s - loss: 2.4617e-05\n",
      "Epoch 89/100\n",
      " - 2s - loss: 2.4617e-05\n",
      "Epoch 90/100\n",
      " - 2s - loss: 2.4617e-05\n",
      "Epoch 91/100\n",
      " - 2s - loss: 2.4617e-05\n",
      "Epoch 92/100\n",
      " - 2s - loss: 2.4617e-05\n",
      "Epoch 93/100\n",
      " - 2s - loss: 2.4617e-05\n",
      "Epoch 94/100\n",
      " - 2s - loss: 2.4617e-05\n",
      "Epoch 95/100\n",
      " - 2s - loss: 2.4617e-05\n",
      "Epoch 96/100\n",
      " - 2s - loss: 2.4617e-05\n",
      "Epoch 97/100\n",
      " - 2s - loss: 2.4617e-05\n",
      "Epoch 98/100\n",
      " - 2s - loss: 2.4617e-05\n",
      "Epoch 99/100\n",
      " - 2s - loss: 2.4617e-05\n",
      "Epoch 100/100\n",
      " - 2s - loss: 2.4617e-05\n",
      "Train Score: 57.46 RMSE\n",
      "Test Score: 57.48 RMSE\n"
     ]
    }
   ],
   "source": [
    "#Hinge loss function\n",
    "build_model(train,test,1,'tanh','adam',100,'hinge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has an average error of about 57.46 on the training dataset, and about 57.48 on the test dataset. By this we can see that these set of function give out the bad predictions of Log Error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " - 2s - loss: 0.0526\n",
      "Epoch 2/100\n",
      " - 2s - loss: 8.0372e-04\n",
      "Epoch 3/100\n",
      " - 2s - loss: 1.5111e-04\n",
      "Epoch 4/100\n",
      " - 2s - loss: 1.5109e-04\n",
      "Epoch 5/100\n",
      " - 2s - loss: 1.5109e-04\n",
      "Epoch 6/100\n",
      " - 2s - loss: 1.5106e-04\n",
      "Epoch 7/100\n",
      " - 2s - loss: 1.5108e-04\n",
      "Epoch 8/100\n",
      " - 1s - loss: 1.5108e-04\n",
      "Epoch 9/100\n",
      " - 1s - loss: 1.5108e-04\n",
      "Epoch 10/100\n",
      " - 2s - loss: 1.5107e-04\n",
      "Epoch 11/100\n",
      " - 1s - loss: 1.5107e-04\n",
      "Epoch 12/100\n",
      " - 2s - loss: 1.5107e-04\n",
      "Epoch 13/100\n",
      " - 2s - loss: 1.5102e-04\n",
      "Epoch 14/100\n",
      " - 2s - loss: 1.5103e-04\n",
      "Epoch 15/100\n",
      " - 2s - loss: 1.5101e-04\n",
      "Epoch 16/100\n",
      " - 1s - loss: 1.5101e-04\n",
      "Epoch 17/100\n",
      " - 2s - loss: 1.5097e-04\n",
      "Epoch 18/100\n",
      " - 2s - loss: 1.5095e-04\n",
      "Epoch 19/100\n",
      " - 2s - loss: 1.5095e-04\n",
      "Epoch 20/100\n",
      " - 2s - loss: 1.5097e-04\n",
      "Epoch 21/100\n",
      " - 2s - loss: 1.5088e-04\n",
      "Epoch 22/100\n",
      " - 2s - loss: 1.5089e-04\n",
      "Epoch 23/100\n",
      " - 2s - loss: 1.5083e-04\n",
      "Epoch 24/100\n",
      " - 2s - loss: 1.5079e-04\n",
      "Epoch 25/100\n",
      " - 2s - loss: 1.5089e-04\n",
      "Epoch 26/100\n",
      " - 2s - loss: 1.5078e-04\n",
      "Epoch 27/100\n",
      " - 2s - loss: 1.5087e-04\n",
      "Epoch 28/100\n",
      " - 2s - loss: 1.5061e-04\n",
      "Epoch 29/100\n",
      " - 2s - loss: 1.5065e-04\n",
      "Epoch 30/100\n",
      " - 2s - loss: 1.5067e-04\n",
      "Epoch 31/100\n",
      " - 2s - loss: 1.5069e-04\n",
      "Epoch 32/100\n",
      " - 2s - loss: 1.5059e-04\n",
      "Epoch 33/100\n",
      " - 2s - loss: 1.5076e-04\n",
      "Epoch 34/100\n",
      " - 2s - loss: 1.5047e-04\n",
      "Epoch 35/100\n",
      " - 2s - loss: 1.5085e-04\n",
      "Epoch 36/100\n",
      " - 2s - loss: 1.5060e-04\n",
      "Epoch 37/100\n",
      " - 2s - loss: 1.5085e-04\n",
      "Epoch 38/100\n",
      " - 2s - loss: 1.5066e-04\n",
      "Epoch 39/100\n",
      " - 2s - loss: 1.5059e-04\n",
      "Epoch 40/100\n",
      " - 1s - loss: 1.5063e-04\n",
      "Epoch 41/100\n",
      " - 2s - loss: 1.5061e-04\n",
      "Epoch 42/100\n",
      " - 2s - loss: 1.5054e-04\n",
      "Epoch 43/100\n",
      " - 2s - loss: 1.5049e-04\n",
      "Epoch 44/100\n",
      " - 2s - loss: 1.5048e-04\n",
      "Epoch 45/100\n",
      " - 2s - loss: 1.5065e-04\n",
      "Epoch 46/100\n",
      " - 2s - loss: 1.5046e-04\n",
      "Epoch 47/100\n",
      " - 2s - loss: 1.5068e-04\n",
      "Epoch 48/100\n",
      " - 2s - loss: 1.5049e-04\n",
      "Epoch 49/100\n",
      " - 2s - loss: 1.5038e-04\n",
      "Epoch 50/100\n",
      " - 1s - loss: 1.5062e-04\n",
      "Epoch 51/100\n",
      " - 1s - loss: 1.5078e-04\n",
      "Epoch 52/100\n",
      " - 2s - loss: 1.5062e-04\n",
      "Epoch 53/100\n",
      " - 2s - loss: 1.5042e-04\n",
      "Epoch 54/100\n",
      " - 1s - loss: 1.5059e-04\n",
      "Epoch 55/100\n",
      " - 2s - loss: 1.5050e-04\n",
      "Epoch 56/100\n",
      " - 2s - loss: 1.5081e-04\n",
      "Epoch 57/100\n",
      " - 2s - loss: 1.5064e-04\n",
      "Epoch 58/100\n",
      " - 2s - loss: 1.5058e-04\n",
      "Epoch 59/100\n",
      " - 2s - loss: 1.5058e-04\n",
      "Epoch 60/100\n",
      " - 2s - loss: 1.5045e-04\n",
      "Epoch 61/100\n",
      " - 2s - loss: 1.5064e-04\n",
      "Epoch 62/100\n",
      " - 2s - loss: 1.5053e-04\n",
      "Epoch 63/100\n",
      " - 2s - loss: 1.5054e-04\n",
      "Epoch 64/100\n",
      " - 2s - loss: 1.5050e-04\n",
      "Epoch 65/100\n",
      " - 2s - loss: 1.5040e-04\n",
      "Epoch 66/100\n",
      " - 2s - loss: 1.5074e-04\n",
      "Epoch 67/100\n",
      " - 2s - loss: 1.5045e-04\n",
      "Epoch 68/100\n",
      " - 2s - loss: 1.5064e-04\n",
      "Epoch 69/100\n",
      " - 2s - loss: 1.5066e-04\n",
      "Epoch 70/100\n",
      " - 2s - loss: 1.5054e-04\n",
      "Epoch 71/100\n",
      " - 2s - loss: 1.5069e-04\n",
      "Epoch 72/100\n",
      " - 2s - loss: 1.5055e-04\n",
      "Epoch 73/100\n",
      " - 2s - loss: 1.5044e-04\n",
      "Epoch 74/100\n",
      " - 2s - loss: 1.5077e-04\n",
      "Epoch 75/100\n",
      " - 2s - loss: 1.5054e-04\n",
      "Epoch 76/100\n",
      " - 2s - loss: 1.5060e-04\n",
      "Epoch 77/100\n",
      " - 2s - loss: 1.5073e-04\n",
      "Epoch 78/100\n",
      " - 1s - loss: 1.5058e-04\n",
      "Epoch 79/100\n",
      " - 2s - loss: 1.5077e-04\n",
      "Epoch 80/100\n",
      " - 2s - loss: 1.5048e-04\n",
      "Epoch 81/100\n",
      " - 2s - loss: 1.5046e-04\n",
      "Epoch 82/100\n",
      " - 2s - loss: 1.5055e-04\n",
      "Epoch 83/100\n",
      " - 2s - loss: 1.5048e-04\n",
      "Epoch 84/100\n",
      " - 2s - loss: 1.5058e-04\n",
      "Epoch 85/100\n",
      " - 2s - loss: 1.5055e-04\n",
      "Epoch 86/100\n",
      " - 2s - loss: 1.5055e-04\n",
      "Epoch 87/100\n",
      " - 2s - loss: 1.5056e-04\n",
      "Epoch 88/100\n",
      " - 2s - loss: 1.5051e-04\n",
      "Epoch 89/100\n",
      " - 2s - loss: 1.5051e-04\n",
      "Epoch 90/100\n",
      " - 1s - loss: 1.5072e-04\n",
      "Epoch 91/100\n",
      " - 1s - loss: 1.5072e-04\n",
      "Epoch 92/100\n",
      " - 1s - loss: 1.5051e-04\n",
      "Epoch 93/100\n",
      " - 2s - loss: 1.5045e-04\n",
      "Epoch 94/100\n",
      " - 1s - loss: 1.5067e-04\n",
      "Epoch 95/100\n",
      " - 2s - loss: 1.5054e-04\n",
      "Epoch 96/100\n",
      " - 2s - loss: 1.5062e-04\n",
      "Epoch 97/100\n",
      " - 2s - loss: 1.5085e-04\n",
      "Epoch 98/100\n",
      " - 2s - loss: 1.5061e-04\n",
      "Epoch 99/100\n",
      " - 1s - loss: 1.5066e-04\n",
      "Epoch 100/100\n",
      " - 2s - loss: 1.5034e-04\n",
      "Train Score: 0.16 RMSE\n",
      "Test Score: 0.15 RMSE\n"
     ]
    }
   ],
   "source": [
    "#logcosh loss function\n",
    "build_model(train,test,1,'tanh','adam',100,'logcosh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has an average error of about 0.16 on the training dataset, and about 0.15 on the test dataset. By this we can see that these set of function give out the same predictions of Log Error as the mean squared error loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " - 2s - loss: 0.6602\n",
      "Epoch 2/100\n",
      " - 2s - loss: 0.1939\n",
      "Epoch 3/100\n",
      " - 2s - loss: 0.0024\n",
      "Epoch 4/100\n",
      " - 2s - loss: 0.0015\n",
      "Epoch 5/100\n",
      " - 2s - loss: 0.0012\n",
      "Epoch 6/100\n",
      " - 2s - loss: 0.0011\n",
      "Epoch 7/100\n",
      " - 2s - loss: 0.0010\n",
      "Epoch 8/100\n",
      " - 2s - loss: 9.4884e-04\n",
      "Epoch 9/100\n",
      " - 2s - loss: 8.9868e-04\n",
      "Epoch 10/100\n",
      " - 2s - loss: 8.5881e-04\n",
      "Epoch 11/100\n",
      " - 2s - loss: 8.2596e-04\n",
      "Epoch 12/100\n",
      " - 2s - loss: 7.9799e-04\n",
      "Epoch 13/100\n",
      " - 2s - loss: 7.7324e-04\n",
      "Epoch 14/100\n",
      " - 2s - loss: 7.5099e-04\n",
      "Epoch 15/100\n",
      " - 2s - loss: 7.3112e-04\n",
      "Epoch 16/100\n",
      " - 2s - loss: 7.1346e-04\n",
      "Epoch 17/100\n",
      " - 2s - loss: 6.9726e-04\n",
      "Epoch 18/100\n",
      " - 2s - loss: 6.8242e-04\n",
      "Epoch 19/100\n",
      " - 2s - loss: 6.6879e-04\n",
      "Epoch 20/100\n",
      " - 2s - loss: 6.5608e-04\n",
      "Epoch 21/100\n",
      " - 2s - loss: 6.4411e-04\n",
      "Epoch 22/100\n",
      " - 2s - loss: 6.3287e-04\n",
      "Epoch 23/100\n",
      " - 2s - loss: 6.2235e-04\n",
      "Epoch 24/100\n",
      " - 2s - loss: 6.1249e-04\n",
      "Epoch 25/100\n",
      " - 2s - loss: 6.0313e-04\n",
      "Epoch 26/100\n",
      " - 2s - loss: 5.9431e-04\n",
      "Epoch 27/100\n",
      " - 2s - loss: 5.8590e-04\n",
      "Epoch 28/100\n",
      " - 2s - loss: 5.7789e-04\n",
      "Epoch 29/100\n",
      " - 2s - loss: 5.7020e-04\n",
      "Epoch 30/100\n",
      " - 2s - loss: 5.6291e-04\n",
      "Epoch 31/100\n",
      " - 2s - loss: 5.5588e-04\n",
      "Epoch 32/100\n",
      " - 2s - loss: 5.4899e-04\n",
      "Epoch 33/100\n",
      " - 2s - loss: 5.4222e-04\n",
      "Epoch 34/100\n",
      " - 2s - loss: 5.3571e-04\n",
      "Epoch 35/100\n",
      " - 2s - loss: 5.2929e-04\n",
      "Epoch 36/100\n",
      " - 2s - loss: 5.2306e-04\n",
      "Epoch 37/100\n",
      " - 2s - loss: 5.1709e-04\n",
      "Epoch 38/100\n",
      " - 2s - loss: 5.1138e-04\n",
      "Epoch 39/100\n",
      " - 2s - loss: 5.0580e-04\n",
      "Epoch 40/100\n",
      " - 2s - loss: 5.0032e-04\n",
      "Epoch 41/100\n",
      " - 2s - loss: 4.9514e-04\n",
      "Epoch 42/100\n",
      " - 2s - loss: 4.9016e-04\n",
      "Epoch 43/100\n",
      " - 2s - loss: 4.8527e-04\n",
      "Epoch 44/100\n",
      " - 2s - loss: 4.8051e-04\n",
      "Epoch 45/100\n",
      " - 2s - loss: 4.7591e-04\n",
      "Epoch 46/100\n",
      " - 2s - loss: 4.7143e-04\n",
      "Epoch 47/100\n",
      " - 2s - loss: 4.6712e-04\n",
      "Epoch 48/100\n",
      " - 2s - loss: 4.6300e-04\n",
      "Epoch 49/100\n",
      " - 2s - loss: 4.5908e-04\n",
      "Epoch 50/100\n",
      " - 2s - loss: 4.5532e-04\n",
      "Epoch 51/100\n",
      " - 2s - loss: 4.5162e-04\n",
      "Epoch 52/100\n",
      " - 2s - loss: 4.4803e-04\n",
      "Epoch 53/100\n",
      " - 2s - loss: 4.4453e-04\n",
      "Epoch 54/100\n",
      " - 1s - loss: 4.4104e-04\n",
      "Epoch 55/100\n",
      " - 2s - loss: 4.3764e-04\n",
      "Epoch 56/100\n",
      " - 2s - loss: 4.3432e-04\n",
      "Epoch 57/100\n",
      " - 2s - loss: 4.3110e-04\n",
      "Epoch 58/100\n",
      " - 2s - loss: 4.2796e-04\n",
      "Epoch 59/100\n",
      " - 2s - loss: 4.2496e-04\n",
      "Epoch 60/100\n",
      " - 2s - loss: 4.2208e-04\n",
      "Epoch 61/100\n",
      " - 2s - loss: 4.1930e-04\n",
      "Epoch 62/100\n",
      " - 2s - loss: 4.1661e-04\n",
      "Epoch 63/100\n",
      " - 2s - loss: 4.1401e-04\n",
      "Epoch 64/100\n",
      " - 2s - loss: 4.1152e-04\n",
      "Epoch 65/100\n",
      " - 2s - loss: 4.0915e-04\n",
      "Epoch 66/100\n",
      " - 2s - loss: 4.0688e-04\n",
      "Epoch 67/100\n",
      " - 1s - loss: 4.0464e-04\n",
      "Epoch 68/100\n",
      " - 2s - loss: 4.0244e-04\n",
      "Epoch 69/100\n",
      " - 2s - loss: 4.0033e-04\n",
      "Epoch 70/100\n",
      " - 2s - loss: 3.9832e-04\n",
      "Epoch 71/100\n",
      " - 2s - loss: 3.9635e-04\n",
      "Epoch 72/100\n",
      " - 2s - loss: 3.9441e-04\n",
      "Epoch 73/100\n",
      " - 2s - loss: 3.9252e-04\n",
      "Epoch 74/100\n",
      " - 2s - loss: 3.9071e-04\n",
      "Epoch 75/100\n",
      " - 2s - loss: 3.8889e-04\n",
      "Epoch 76/100\n",
      " - 2s - loss: 3.8713e-04\n",
      "Epoch 77/100\n",
      " - 2s - loss: 3.8543e-04\n",
      "Epoch 78/100\n",
      " - 2s - loss: 3.8381e-04\n",
      "Epoch 79/100\n",
      " - 2s - loss: 3.8223e-04\n",
      "Epoch 80/100\n",
      " - 2s - loss: 3.8068e-04\n",
      "Epoch 81/100\n",
      " - 2s - loss: 3.7916e-04\n",
      "Epoch 82/100\n",
      " - 2s - loss: 3.7769e-04\n",
      "Epoch 83/100\n",
      " - 2s - loss: 3.7626e-04\n",
      "Epoch 84/100\n",
      " - 2s - loss: 3.7486e-04\n",
      "Epoch 85/100\n",
      " - 2s - loss: 3.7347e-04\n",
      "Epoch 86/100\n",
      " - 2s - loss: 3.7208e-04\n",
      "Epoch 87/100\n",
      " - 2s - loss: 3.7070e-04\n",
      "Epoch 88/100\n",
      " - 2s - loss: 3.6934e-04\n",
      "Epoch 89/100\n",
      " - 2s - loss: 3.6803e-04\n",
      "Epoch 90/100\n",
      " - 2s - loss: 3.6675e-04\n",
      "Epoch 91/100\n",
      " - 2s - loss: 3.6549e-04\n",
      "Epoch 92/100\n",
      " - 2s - loss: 3.6424e-04\n",
      "Epoch 93/100\n",
      " - 2s - loss: 3.6299e-04\n",
      "Epoch 94/100\n",
      " - 2s - loss: 3.6176e-04\n",
      "Epoch 95/100\n",
      " - 2s - loss: 3.6056e-04\n",
      "Epoch 96/100\n",
      " - 2s - loss: 3.5936e-04\n",
      "Epoch 97/100\n",
      " - 2s - loss: 3.5816e-04\n",
      "Epoch 98/100\n",
      " - 2s - loss: 3.5699e-04\n",
      "Epoch 99/100\n",
      " - 2s - loss: 3.5587e-04\n",
      "Epoch 100/100\n",
      " - 2s - loss: 3.5476e-04\n",
      "Train Score: 18.40 RMSE\n",
      "Test Score: 18.40 RMSE\n"
     ]
    }
   ],
   "source": [
    "build_model(train,test,1,'tanh','Adagrad',100,'hinge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has an average error of about 18.40 on the training dataset, and about 18.40 on the test dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      " - 3s - loss: 0.7224\n",
      "Epoch 2/200\n",
      " - 2s - loss: 0.4858\n",
      "Epoch 3/200\n",
      " - 2s - loss: 0.4538\n",
      "Epoch 4/200\n",
      " - 2s - loss: 0.4306\n",
      "Epoch 5/200\n",
      " - 2s - loss: 0.4079\n",
      "Epoch 6/200\n",
      " - 2s - loss: 0.3889\n",
      "Epoch 7/200\n",
      " - 2s - loss: 0.3696\n",
      "Epoch 8/200\n",
      " - 2s - loss: 0.3541\n",
      "Epoch 9/200\n",
      " - 2s - loss: 0.3363\n",
      "Epoch 10/200\n",
      " - 2s - loss: 0.3219\n",
      "Epoch 11/200\n",
      " - 2s - loss: 0.3057\n",
      "Epoch 12/200\n",
      " - 2s - loss: 0.2919\n",
      "Epoch 13/200\n",
      " - 2s - loss: 0.2771\n",
      "Epoch 14/200\n",
      " - 2s - loss: 0.2634\n",
      "Epoch 15/200\n",
      " - 2s - loss: 0.2504\n",
      "Epoch 16/200\n",
      " - 2s - loss: 0.2387\n",
      "Epoch 17/200\n",
      " - 2s - loss: 0.2258\n",
      "Epoch 18/200\n",
      " - 2s - loss: 0.2129\n",
      "Epoch 19/200\n",
      " - 2s - loss: 0.2021\n",
      "Epoch 20/200\n",
      " - 2s - loss: 0.1903\n",
      "Epoch 21/200\n",
      " - 2s - loss: 0.1795\n",
      "Epoch 22/200\n",
      " - 2s - loss: 0.1677\n",
      "Epoch 23/200\n",
      " - 2s - loss: 0.1571\n",
      "Epoch 24/200\n",
      " - 2s - loss: 0.1466\n",
      "Epoch 25/200\n",
      " - 2s - loss: 0.1365\n",
      "Epoch 26/200\n",
      " - 2s - loss: 0.1259\n",
      "Epoch 27/200\n",
      " - 2s - loss: 0.1166\n",
      "Epoch 28/200\n",
      " - 2s - loss: 0.1076\n",
      "Epoch 29/200\n",
      " - 2s - loss: 0.0974\n",
      "Epoch 30/200\n",
      " - 2s - loss: 0.0877\n",
      "Epoch 31/200\n",
      " - 2s - loss: 0.0786\n",
      "Epoch 32/200\n",
      " - 2s - loss: 0.0694\n",
      "Epoch 33/200\n",
      " - 2s - loss: 0.0610\n",
      "Epoch 34/200\n",
      " - 2s - loss: 0.0522\n",
      "Epoch 35/200\n",
      " - 2s - loss: 0.0437\n",
      "Epoch 36/200\n",
      " - 2s - loss: 0.0353\n",
      "Epoch 37/200\n",
      " - 2s - loss: 0.0274\n",
      "Epoch 38/200\n",
      " - 2s - loss: 0.0195\n",
      "Epoch 39/200\n",
      " - 2s - loss: 0.0126\n",
      "Epoch 40/200\n",
      " - 2s - loss: 0.0070\n",
      "Epoch 41/200\n",
      " - 2s - loss: 0.0039\n",
      "Epoch 42/200\n",
      " - 2s - loss: 0.0028\n",
      "Epoch 43/200\n",
      " - 2s - loss: 0.0024\n",
      "Epoch 44/200\n",
      " - 2s - loss: 0.0021\n",
      "Epoch 45/200\n",
      " - 2s - loss: 0.0019\n",
      "Epoch 46/200\n",
      " - 2s - loss: 0.0018\n",
      "Epoch 47/200\n",
      " - 2s - loss: 0.0017\n",
      "Epoch 48/200\n",
      " - 2s - loss: 0.0016\n",
      "Epoch 49/200\n",
      " - 2s - loss: 0.0015\n",
      "Epoch 50/200\n",
      " - 2s - loss: 0.0015\n",
      "Epoch 51/200\n",
      " - 2s - loss: 0.0014\n",
      "Epoch 52/200\n",
      " - 2s - loss: 0.0014\n",
      "Epoch 53/200\n",
      " - 2s - loss: 0.0014\n",
      "Epoch 54/200\n",
      " - 2s - loss: 0.0013\n",
      "Epoch 55/200\n",
      " - 2s - loss: 0.0013\n",
      "Epoch 56/200\n",
      " - 2s - loss: 0.0012\n",
      "Epoch 57/200\n",
      " - 2s - loss: 0.0013\n",
      "Epoch 58/200\n",
      " - 2s - loss: 0.0012\n",
      "Epoch 59/200\n",
      " - 2s - loss: 0.0012\n",
      "Epoch 60/200\n",
      " - 2s - loss: 0.0012\n",
      "Epoch 61/200\n",
      " - 2s - loss: 0.0012\n",
      "Epoch 62/200\n",
      " - 2s - loss: 0.0012\n",
      "Epoch 63/200\n",
      " - 2s - loss: 0.0011\n",
      "Epoch 64/200\n",
      " - 2s - loss: 0.0011\n",
      "Epoch 65/200\n",
      " - 2s - loss: 0.0011\n",
      "Epoch 66/200\n",
      " - 2s - loss: 0.0011\n",
      "Epoch 67/200\n",
      " - 2s - loss: 0.0011\n",
      "Epoch 68/200\n",
      " - 2s - loss: 0.0010\n",
      "Epoch 69/200\n",
      " - 2s - loss: 0.0011\n",
      "Epoch 70/200\n",
      " - 2s - loss: 9.9078e-04\n",
      "Epoch 71/200\n",
      " - 2s - loss: 0.0010\n",
      "Epoch 72/200\n",
      " - 2s - loss: 9.9873e-04\n",
      "Epoch 73/200\n",
      " - 2s - loss: 9.9247e-04\n",
      "Epoch 74/200\n",
      " - 2s - loss: 9.7983e-04\n",
      "Epoch 75/200\n",
      " - 2s - loss: 9.8080e-04\n",
      "Epoch 76/200\n",
      " - 2s - loss: 9.9915e-04\n",
      "Epoch 77/200\n",
      " - 2s - loss: 9.2192e-04\n",
      "Epoch 78/200\n",
      " - 2s - loss: 9.4335e-04\n",
      "Epoch 79/200\n",
      " - 2s - loss: 9.2506e-04\n",
      "Epoch 80/200\n",
      " - 2s - loss: 9.7573e-04\n",
      "Epoch 81/200\n",
      " - 2s - loss: 9.2763e-04\n",
      "Epoch 82/200\n",
      " - 2s - loss: 8.8939e-04\n",
      "Epoch 83/200\n",
      " - 2s - loss: 8.8042e-04\n",
      "Epoch 84/200\n",
      " - 2s - loss: 8.9681e-04\n",
      "Epoch 85/200\n",
      " - 2s - loss: 8.6515e-04\n",
      "Epoch 86/200\n",
      " - 2s - loss: 8.5086e-04\n",
      "Epoch 87/200\n",
      " - 2s - loss: 8.7237e-04\n",
      "Epoch 88/200\n",
      " - 2s - loss: 9.1367e-04\n",
      "Epoch 89/200\n",
      " - 2s - loss: 8.7156e-04\n",
      "Epoch 90/200\n",
      " - 2s - loss: 9.1137e-04\n",
      "Epoch 91/200\n",
      " - 2s - loss: 8.1631e-04\n",
      "Epoch 92/200\n",
      " - 2s - loss: 8.4791e-04\n",
      "Epoch 93/200\n",
      " - 2s - loss: 8.3258e-04\n",
      "Epoch 94/200\n",
      " - 2s - loss: 8.2967e-04\n",
      "Epoch 95/200\n",
      " - 2s - loss: 8.2278e-04\n",
      "Epoch 96/200\n",
      " - 2s - loss: 8.5032e-04\n",
      "Epoch 97/200\n",
      " - 2s - loss: 8.1907e-04\n",
      "Epoch 98/200\n",
      " - 2s - loss: 8.1717e-04\n",
      "Epoch 99/200\n",
      " - 2s - loss: 7.8528e-04\n",
      "Epoch 100/200\n",
      " - 2s - loss: 8.2431e-04\n",
      "Epoch 101/200\n",
      " - 2s - loss: 8.2787e-04\n",
      "Epoch 102/200\n",
      " - 2s - loss: 7.7427e-04\n",
      "Epoch 103/200\n",
      " - 2s - loss: 7.7493e-04\n",
      "Epoch 104/200\n",
      " - 2s - loss: 7.7064e-04\n",
      "Epoch 105/200\n",
      " - 2s - loss: 7.2944e-04\n",
      "Epoch 106/200\n",
      " - 2s - loss: 7.4154e-04\n",
      "Epoch 107/200\n",
      " - 2s - loss: 7.8213e-04\n",
      "Epoch 108/200\n",
      " - 2s - loss: 7.5651e-04\n",
      "Epoch 109/200\n",
      " - 2s - loss: 7.9468e-04\n",
      "Epoch 110/200\n",
      " - 2s - loss: 7.7278e-04\n",
      "Epoch 111/200\n",
      " - 2s - loss: 7.2461e-04\n",
      "Epoch 112/200\n",
      " - 2s - loss: 7.8985e-04\n",
      "Epoch 113/200\n",
      " - 2s - loss: 7.6487e-04\n",
      "Epoch 114/200\n",
      " - 2s - loss: 7.4152e-04\n",
      "Epoch 115/200\n",
      " - 2s - loss: 7.6999e-04\n",
      "Epoch 116/200\n",
      " - 2s - loss: 7.9116e-04\n",
      "Epoch 117/200\n",
      " - 2s - loss: 7.1381e-04\n",
      "Epoch 118/200\n",
      " - 2s - loss: 7.0127e-04\n",
      "Epoch 119/200\n",
      " - 2s - loss: 7.3802e-04\n",
      "Epoch 120/200\n",
      " - 2s - loss: 7.2913e-04\n",
      "Epoch 121/200\n",
      " - 2s - loss: 7.3128e-04\n",
      "Epoch 122/200\n",
      " - 2s - loss: 7.0079e-04\n",
      "Epoch 123/200\n",
      " - 2s - loss: 7.1681e-04\n",
      "Epoch 124/200\n",
      " - 2s - loss: 7.1133e-04\n",
      "Epoch 125/200\n",
      " - 2s - loss: 7.0520e-04\n",
      "Epoch 126/200\n",
      " - 2s - loss: 6.9183e-04\n",
      "Epoch 127/200\n",
      " - 2s - loss: 7.1805e-04\n",
      "Epoch 128/200\n",
      " - 2s - loss: 6.9226e-04\n",
      "Epoch 129/200\n",
      " - 2s - loss: 7.0460e-04\n",
      "Epoch 130/200\n",
      " - 2s - loss: 7.4504e-04\n",
      "Epoch 131/200\n",
      " - 2s - loss: 7.1499e-04\n",
      "Epoch 132/200\n",
      " - 2s - loss: 7.0335e-04\n",
      "Epoch 133/200\n",
      " - 2s - loss: 7.0896e-04\n",
      "Epoch 134/200\n",
      " - 2s - loss: 6.6013e-04\n",
      "Epoch 135/200\n",
      " - 2s - loss: 6.9168e-04\n",
      "Epoch 136/200\n",
      " - 2s - loss: 6.6807e-04\n",
      "Epoch 137/200\n",
      " - 2s - loss: 6.9809e-04\n",
      "Epoch 138/200\n",
      " - 2s - loss: 6.9564e-04\n",
      "Epoch 139/200\n",
      " - 2s - loss: 6.6012e-04\n",
      "Epoch 140/200\n",
      " - 2s - loss: 6.8327e-04\n",
      "Epoch 141/200\n",
      " - 2s - loss: 6.4153e-04\n",
      "Epoch 142/200\n",
      " - 2s - loss: 6.8230e-04\n",
      "Epoch 143/200\n",
      " - 2s - loss: 6.7884e-04\n",
      "Epoch 144/200\n",
      " - 2s - loss: 6.5855e-04\n",
      "Epoch 145/200\n",
      " - 2s - loss: 7.0801e-04\n",
      "Epoch 146/200\n",
      " - 2s - loss: 6.9391e-04\n",
      "Epoch 147/200\n",
      " - 2s - loss: 6.6419e-04\n",
      "Epoch 148/200\n",
      " - 2s - loss: 6.6850e-04\n",
      "Epoch 149/200\n",
      " - 2s - loss: 6.7173e-04\n",
      "Epoch 150/200\n",
      " - 2s - loss: 6.7681e-04\n",
      "Epoch 151/200\n",
      " - 2s - loss: 6.5421e-04\n",
      "Epoch 152/200\n",
      " - 2s - loss: 6.4516e-04\n",
      "Epoch 153/200\n",
      " - 2s - loss: 6.3977e-04\n",
      "Epoch 154/200\n",
      " - 2s - loss: 6.1646e-04\n",
      "Epoch 155/200\n",
      " - 2s - loss: 6.2357e-04\n",
      "Epoch 156/200\n",
      " - 2s - loss: 6.9226e-04\n",
      "Epoch 157/200\n",
      " - 2s - loss: 6.6721e-04\n",
      "Epoch 158/200\n",
      " - 2s - loss: 6.4601e-04\n",
      "Epoch 159/200\n",
      " - 2s - loss: 6.1131e-04\n",
      "Epoch 160/200\n",
      " - 2s - loss: 6.3982e-04\n",
      "Epoch 161/200\n",
      " - 2s - loss: 6.5175e-04\n",
      "Epoch 162/200\n",
      " - 2s - loss: 6.2716e-04\n",
      "Epoch 163/200\n",
      " - 2s - loss: 5.7568e-04\n",
      "Epoch 164/200\n",
      " - 2s - loss: 6.2673e-04\n",
      "Epoch 165/200\n",
      " - 2s - loss: 6.5174e-04\n",
      "Epoch 166/200\n",
      " - 2s - loss: 6.1602e-04\n",
      "Epoch 167/200\n",
      " - 2s - loss: 5.8113e-04\n",
      "Epoch 168/200\n",
      " - 2s - loss: 6.3285e-04\n",
      "Epoch 169/200\n",
      " - 2s - loss: 6.4899e-04\n",
      "Epoch 170/200\n",
      " - 2s - loss: 6.4080e-04\n",
      "Epoch 171/200\n",
      " - 2s - loss: 6.1809e-04\n",
      "Epoch 172/200\n",
      " - 2s - loss: 6.2498e-04\n",
      "Epoch 173/200\n",
      " - 2s - loss: 6.2631e-04\n",
      "Epoch 174/200\n",
      " - 2s - loss: 5.9915e-04\n",
      "Epoch 175/200\n",
      " - 2s - loss: 5.9098e-04\n",
      "Epoch 176/200\n",
      " - 2s - loss: 6.0760e-04\n",
      "Epoch 177/200\n",
      " - 2s - loss: 6.2942e-04\n",
      "Epoch 178/200\n",
      " - 2s - loss: 5.8888e-04\n",
      "Epoch 179/200\n",
      " - 2s - loss: 6.0343e-04\n",
      "Epoch 180/200\n",
      " - 2s - loss: 6.1212e-04\n",
      "Epoch 181/200\n",
      " - 2s - loss: 6.1372e-04\n",
      "Epoch 182/200\n",
      " - 2s - loss: 6.3289e-04\n",
      "Epoch 183/200\n",
      " - 2s - loss: 5.9967e-04\n",
      "Epoch 184/200\n",
      " - 2s - loss: 6.0979e-04\n",
      "Epoch 185/200\n",
      " - 2s - loss: 5.9573e-04\n",
      "Epoch 186/200\n",
      " - 2s - loss: 6.2092e-04\n",
      "Epoch 187/200\n",
      " - 2s - loss: 5.8732e-04\n",
      "Epoch 188/200\n",
      " - 2s - loss: 5.9009e-04\n",
      "Epoch 189/200\n",
      " - 2s - loss: 6.0653e-04\n",
      "Epoch 190/200\n",
      " - 2s - loss: 6.3260e-04\n",
      "Epoch 191/200\n",
      " - 2s - loss: 6.1116e-04\n",
      "Epoch 192/200\n",
      " - 2s - loss: 5.8734e-04\n",
      "Epoch 193/200\n",
      " - 2s - loss: 5.8908e-04\n",
      "Epoch 194/200\n",
      " - 2s - loss: 6.0421e-04\n",
      "Epoch 195/200\n",
      " - 2s - loss: 6.0563e-04\n",
      "Epoch 196/200\n",
      " - 2s - loss: 5.8372e-04\n",
      "Epoch 197/200\n",
      " - 2s - loss: 5.6278e-04\n",
      "Epoch 198/200\n",
      " - 2s - loss: 6.1228e-04\n",
      "Epoch 199/200\n",
      " - 2s - loss: 5.8979e-04\n",
      "Epoch 200/200\n",
      " - 2s - loss: 5.9716e-04\n",
      "Train Score: 22.20 RMSE\n",
      "Test Score: 22.19 RMSE\n"
     ]
    }
   ],
   "source": [
    "look_back = 1\n",
    "trainX, trainY = create_dataset(train, look_back)\n",
    "testX, testY = create_dataset(test, look_back)\n",
    "trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
    "model = Sequential()\n",
    "model.add(LSTM(4, input_shape=(1, look_back),activation = 'tanh'))\n",
    "model.add(Dense(1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(.6))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='hinge', optimizer='Adagrad')\n",
    "model.fit(trainX, trainY, epochs=200, batch_size=256, verbose=2)\n",
    "trainPredict = model.predict(trainX)\n",
    "testPredict = model.predict(testX)\n",
    "trainPredict = scaler.inverse_transform(trainPredict)\n",
    "trainY = scaler.inverse_transform(trainY)\n",
    "testPredict = scaler.inverse_transform(testPredict)\n",
    "testY = scaler.inverse_transform(testY)\n",
    "trainScore = math.sqrt(mean_squared_error(trainY, trainPredict[:, 0]))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "testScore = math.sqrt(mean_squared_error(testY, testPredict[:, 0]))\n",
    "print('Test Score: %.2f RMSE' % (testScore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The eposch has been set as 200 but this doesn't improve the RSME rather the error is increased to 22.20 on train and 22.19 on test data set in comparision with eposch=100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Initializers\n",
    "Initializations define the way to set the initial random weights of Keras layers.\n",
    "\n",
    "#### Normal\n",
    "Initializer that generates tensors with a normal distribution.\n",
    "Here, we have used Random normal distribution of the weights and epoch being 200 the RSME of train dataset 21.52 and test 21.52.\n",
    "\n",
    "#### Random Uniform\n",
    "Initializer that generates tensors with a uniform distribution.\n",
    "Here, we have used Random normal distribution of the weights and epoch being 200 the RSME of train dataset 21.51 and test 21.50.\n",
    "\n",
    "Even with the normal or uniformly distributed weights the error of the log Error doesn't improve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      " - 3s - loss: 0.8240\n",
      "Epoch 2/200\n",
      " - 2s - loss: 0.5971\n",
      "Epoch 3/200\n",
      " - 2s - loss: 0.4466\n",
      "Epoch 4/200\n",
      " - 2s - loss: 0.4187\n",
      "Epoch 5/200\n",
      " - 2s - loss: 0.4024\n",
      "Epoch 6/200\n",
      " - 2s - loss: 0.3841\n",
      "Epoch 7/200\n",
      " - 2s - loss: 0.3659\n",
      "Epoch 8/200\n",
      " - 2s - loss: 0.3511\n",
      "Epoch 9/200\n",
      " - 2s - loss: 0.3362\n",
      "Epoch 10/200\n",
      " - 2s - loss: 0.3195\n",
      "Epoch 11/200\n",
      " - 2s - loss: 0.3078\n",
      "Epoch 12/200\n",
      " - 2s - loss: 0.2922\n",
      "Epoch 13/200\n",
      " - 2s - loss: 0.2816\n",
      "Epoch 14/200\n",
      " - 2s - loss: 0.2678\n",
      "Epoch 15/200\n",
      " - 2s - loss: 0.2551\n",
      "Epoch 16/200\n",
      " - 2s - loss: 0.2428\n",
      "Epoch 17/200\n",
      " - 2s - loss: 0.2315\n",
      "Epoch 18/200\n",
      " - 2s - loss: 0.2205\n",
      "Epoch 19/200\n",
      " - 2s - loss: 0.2086\n",
      "Epoch 20/200\n",
      " - 2s - loss: 0.1989\n",
      "Epoch 21/200\n",
      " - 2s - loss: 0.1872\n",
      "Epoch 22/200\n",
      " - 2s - loss: 0.1767\n",
      "Epoch 23/200\n",
      " - 2s - loss: 0.1659\n",
      "Epoch 24/200\n",
      " - 2s - loss: 0.1551\n",
      "Epoch 25/200\n",
      " - 2s - loss: 0.1457\n",
      "Epoch 26/200\n",
      " - 2s - loss: 0.1365\n",
      "Epoch 27/200\n",
      " - 2s - loss: 0.1273\n",
      "Epoch 28/200\n",
      " - 2s - loss: 0.1169\n",
      "Epoch 29/200\n",
      " - 2s - loss: 0.1082\n",
      "Epoch 30/200\n",
      " - 2s - loss: 0.0991\n",
      "Epoch 31/200\n",
      " - 2s - loss: 0.0900\n",
      "Epoch 32/200\n",
      " - 2s - loss: 0.0814\n",
      "Epoch 33/200\n",
      " - 2s - loss: 0.0727\n",
      "Epoch 34/200\n",
      " - 2s - loss: 0.0641\n",
      "Epoch 35/200\n",
      " - 2s - loss: 0.0558\n",
      "Epoch 36/200\n",
      " - 2s - loss: 0.0476\n",
      "Epoch 37/200\n",
      " - 2s - loss: 0.0394\n",
      "Epoch 38/200\n",
      " - 2s - loss: 0.0317\n",
      "Epoch 39/200\n",
      " - 2s - loss: 0.0239\n",
      "Epoch 40/200\n",
      " - 2s - loss: 0.0169\n",
      "Epoch 41/200\n",
      " - 2s - loss: 0.0104\n",
      "Epoch 42/200\n",
      " - 2s - loss: 0.0057\n",
      "Epoch 43/200\n",
      " - 2s - loss: 0.0035\n",
      "Epoch 44/200\n",
      " - 2s - loss: 0.0027\n",
      "Epoch 45/200\n",
      " - 2s - loss: 0.0024\n",
      "Epoch 46/200\n",
      " - 2s - loss: 0.0021\n",
      "Epoch 47/200\n",
      " - 2s - loss: 0.0019\n",
      "Epoch 48/200\n",
      " - 2s - loss: 0.0018\n",
      "Epoch 49/200\n",
      " - 2s - loss: 0.0017\n",
      "Epoch 50/200\n",
      " - 2s - loss: 0.0016\n",
      "Epoch 51/200\n",
      " - 2s - loss: 0.0016\n",
      "Epoch 52/200\n",
      " - 2s - loss: 0.0015\n",
      "Epoch 53/200\n",
      " - 2s - loss: 0.0014\n",
      "Epoch 54/200\n",
      " - 2s - loss: 0.0014\n",
      "Epoch 55/200\n",
      " - 2s - loss: 0.0014\n",
      "Epoch 56/200\n",
      " - 2s - loss: 0.0014\n",
      "Epoch 57/200\n",
      " - 2s - loss: 0.0013\n",
      "Epoch 58/200\n",
      " - 2s - loss: 0.0013\n",
      "Epoch 59/200\n",
      " - 2s - loss: 0.0013\n",
      "Epoch 60/200\n",
      " - 2s - loss: 0.0012\n",
      "Epoch 61/200\n",
      " - 2s - loss: 0.0012\n",
      "Epoch 62/200\n",
      " - 2s - loss: 0.0012\n",
      "Epoch 63/200\n",
      " - 2s - loss: 0.0012\n",
      "Epoch 64/200\n",
      " - 2s - loss: 0.0012\n",
      "Epoch 65/200\n",
      " - 2s - loss: 0.0012\n",
      "Epoch 66/200\n",
      " - 2s - loss: 0.0012\n",
      "Epoch 67/200\n",
      " - 2s - loss: 0.0011\n",
      "Epoch 68/200\n",
      " - 2s - loss: 0.0011\n",
      "Epoch 69/200\n",
      " - 2s - loss: 0.0011\n",
      "Epoch 70/200\n",
      " - 2s - loss: 0.0011\n",
      "Epoch 71/200\n",
      " - 2s - loss: 0.0010\n",
      "Epoch 72/200\n",
      " - 2s - loss: 0.0011\n",
      "Epoch 73/200\n",
      " - 2s - loss: 0.0010\n",
      "Epoch 74/200\n",
      " - 2s - loss: 0.0010\n",
      "Epoch 75/200\n",
      " - 2s - loss: 0.0010\n",
      "Epoch 76/200\n",
      " - 2s - loss: 9.6475e-04\n",
      "Epoch 77/200\n",
      " - 2s - loss: 9.9516e-04\n",
      "Epoch 78/200\n",
      " - 2s - loss: 9.6345e-04\n",
      "Epoch 79/200\n",
      " - 2s - loss: 9.6103e-04\n",
      "Epoch 80/200\n",
      " - 2s - loss: 9.4485e-04\n",
      "Epoch 81/200\n",
      " - 2s - loss: 9.6190e-04\n",
      "Epoch 82/200\n",
      " - 2s - loss: 9.0472e-04\n",
      "Epoch 83/200\n",
      " - 2s - loss: 9.2596e-04\n",
      "Epoch 84/200\n",
      " - 2s - loss: 9.6402e-04\n",
      "Epoch 85/200\n",
      " - 2s - loss: 8.8723e-04\n",
      "Epoch 86/200\n",
      " - 2s - loss: 8.7121e-04\n",
      "Epoch 87/200\n",
      " - 2s - loss: 9.1835e-04\n",
      "Epoch 88/200\n",
      " - 2s - loss: 8.9630e-04\n",
      "Epoch 89/200\n",
      " - 2s - loss: 9.1645e-04\n",
      "Epoch 90/200\n",
      " - 2s - loss: 8.6459e-04\n",
      "Epoch 91/200\n",
      " - 2s - loss: 9.0773e-04\n",
      "Epoch 92/200\n",
      " - 2s - loss: 8.6934e-04\n",
      "Epoch 93/200\n",
      " - 2s - loss: 8.5118e-04\n",
      "Epoch 94/200\n",
      " - 2s - loss: 8.2122e-04\n",
      "Epoch 95/200\n",
      " - 2s - loss: 8.9609e-04\n",
      "Epoch 96/200\n",
      " - 2s - loss: 8.5332e-04\n",
      "Epoch 97/200\n",
      " - 2s - loss: 8.3787e-04\n",
      "Epoch 98/200\n",
      " - 2s - loss: 8.7431e-04\n",
      "Epoch 99/200\n",
      " - 2s - loss: 8.5946e-04\n",
      "Epoch 100/200\n",
      " - 2s - loss: 8.2833e-04\n",
      "Epoch 101/200\n",
      " - 2s - loss: 7.8433e-04\n",
      "Epoch 102/200\n",
      " - 2s - loss: 7.9852e-04\n",
      "Epoch 103/200\n",
      " - 2s - loss: 7.9664e-04\n",
      "Epoch 104/200\n",
      " - 2s - loss: 8.5617e-04\n",
      "Epoch 105/200\n",
      " - 2s - loss: 8.0400e-04\n",
      "Epoch 106/200\n",
      " - 2s - loss: 7.8795e-04\n",
      "Epoch 107/200\n",
      " - 2s - loss: 8.4639e-04\n",
      "Epoch 108/200\n",
      " - 2s - loss: 7.7368e-04\n",
      "Epoch 109/200\n",
      " - 2s - loss: 8.2912e-04\n",
      "Epoch 110/200\n",
      " - 2s - loss: 7.8803e-04\n",
      "Epoch 111/200\n",
      " - 2s - loss: 7.8356e-04\n",
      "Epoch 112/200\n",
      " - 2s - loss: 8.0386e-04\n",
      "Epoch 113/200\n",
      " - 2s - loss: 7.9733e-04\n",
      "Epoch 114/200\n",
      " - 2s - loss: 7.8641e-04\n",
      "Epoch 115/200\n",
      " - 2s - loss: 7.7076e-04\n",
      "Epoch 116/200\n",
      " - 2s - loss: 8.0549e-04\n",
      "Epoch 117/200\n",
      " - 2s - loss: 7.4076e-04\n",
      "Epoch 118/200\n",
      " - 2s - loss: 7.2872e-04\n",
      "Epoch 119/200\n",
      " - 2s - loss: 7.5860e-04\n",
      "Epoch 120/200\n",
      " - 2s - loss: 7.8516e-04\n",
      "Epoch 121/200\n",
      " - 2s - loss: 7.3576e-04\n",
      "Epoch 122/200\n",
      " - 2s - loss: 7.6538e-04\n",
      "Epoch 123/200\n",
      " - 2s - loss: 7.3483e-04\n",
      "Epoch 124/200\n",
      " - 2s - loss: 7.4184e-04\n",
      "Epoch 125/200\n",
      " - 2s - loss: 7.0130e-04\n",
      "Epoch 126/200\n",
      " - 2s - loss: 7.4668e-04\n",
      "Epoch 127/200\n",
      " - 2s - loss: 7.6837e-04\n",
      "Epoch 128/200\n",
      " - 2s - loss: 7.5883e-04\n",
      "Epoch 129/200\n",
      " - 2s - loss: 7.5510e-04\n",
      "Epoch 130/200\n",
      " - 2s - loss: 7.2291e-04\n",
      "Epoch 131/200\n",
      " - 2s - loss: 7.3367e-04\n",
      "Epoch 132/200\n",
      " - 2s - loss: 7.3112e-04\n",
      "Epoch 133/200\n",
      " - 2s - loss: 7.3445e-04\n",
      "Epoch 134/200\n",
      " - 2s - loss: 7.0739e-04\n",
      "Epoch 135/200\n",
      " - 2s - loss: 6.7452e-04\n",
      "Epoch 136/200\n",
      " - 2s - loss: 7.3353e-04\n",
      "Epoch 137/200\n",
      " - 2s - loss: 7.2297e-04\n",
      "Epoch 138/200\n",
      " - 2s - loss: 7.1352e-04\n",
      "Epoch 139/200\n",
      " - 2s - loss: 7.0767e-04\n",
      "Epoch 140/200\n",
      " - 2s - loss: 7.1801e-04\n",
      "Epoch 141/200\n",
      " - 2s - loss: 7.0701e-04\n",
      "Epoch 142/200\n",
      " - 2s - loss: 6.9580e-04\n",
      "Epoch 143/200\n",
      " - 2s - loss: 6.9351e-04\n",
      "Epoch 144/200\n",
      " - 2s - loss: 6.8956e-04\n",
      "Epoch 145/200\n",
      " - 2s - loss: 6.5751e-04\n",
      "Epoch 146/200\n",
      " - 2s - loss: 7.2035e-04\n",
      "Epoch 147/200\n",
      " - 2s - loss: 7.1047e-04\n",
      "Epoch 148/200\n",
      " - 2s - loss: 6.6768e-04\n",
      "Epoch 149/200\n",
      " - 2s - loss: 6.7917e-04\n",
      "Epoch 150/200\n",
      " - 2s - loss: 6.6454e-04\n",
      "Epoch 151/200\n",
      " - 2s - loss: 6.6745e-04\n",
      "Epoch 152/200\n",
      " - 2s - loss: 7.3386e-04\n",
      "Epoch 153/200\n",
      " - 2s - loss: 6.7568e-04\n",
      "Epoch 154/200\n",
      " - 2s - loss: 6.6847e-04\n",
      "Epoch 155/200\n",
      " - 2s - loss: 6.8936e-04\n",
      "Epoch 156/200\n",
      " - 2s - loss: 7.0069e-04\n",
      "Epoch 157/200\n",
      " - 2s - loss: 6.3824e-04\n",
      "Epoch 158/200\n",
      " - 2s - loss: 6.7730e-04\n",
      "Epoch 159/200\n",
      " - 2s - loss: 6.6016e-04\n",
      "Epoch 160/200\n",
      " - 2s - loss: 6.9494e-04\n",
      "Epoch 161/200\n",
      " - 2s - loss: 6.6537e-04\n",
      "Epoch 162/200\n",
      " - 2s - loss: 6.6486e-04\n",
      "Epoch 163/200\n",
      " - 2s - loss: 6.7203e-04\n",
      "Epoch 164/200\n",
      " - 2s - loss: 6.8547e-04\n",
      "Epoch 165/200\n",
      " - 2s - loss: 6.3612e-04\n",
      "Epoch 166/200\n",
      " - 2s - loss: 6.1724e-04\n",
      "Epoch 167/200\n",
      " - 2s - loss: 6.4767e-04\n",
      "Epoch 168/200\n",
      " - 2s - loss: 6.3488e-04\n",
      "Epoch 169/200\n",
      " - 2s - loss: 6.4561e-04\n",
      "Epoch 170/200\n",
      " - 2s - loss: 6.2544e-04\n",
      "Epoch 171/200\n",
      " - 2s - loss: 6.5155e-04\n",
      "Epoch 172/200\n",
      " - 2s - loss: 6.7910e-04\n",
      "Epoch 173/200\n",
      " - 2s - loss: 6.3247e-04\n",
      "Epoch 174/200\n",
      " - 2s - loss: 6.3906e-04\n",
      "Epoch 175/200\n",
      " - 2s - loss: 6.1734e-04\n",
      "Epoch 176/200\n",
      " - 2s - loss: 6.4515e-04\n",
      "Epoch 177/200\n",
      " - 2s - loss: 6.4713e-04\n",
      "Epoch 178/200\n",
      " - 2s - loss: 6.3090e-04\n",
      "Epoch 179/200\n",
      " - 2s - loss: 6.2174e-04\n",
      "Epoch 180/200\n",
      " - 2s - loss: 6.3808e-04\n",
      "Epoch 181/200\n",
      " - 2s - loss: 6.1388e-04\n",
      "Epoch 182/200\n",
      " - 2s - loss: 6.1405e-04\n",
      "Epoch 183/200\n",
      " - 2s - loss: 6.3985e-04\n",
      "Epoch 184/200\n",
      " - 2s - loss: 6.2843e-04\n",
      "Epoch 185/200\n",
      " - 2s - loss: 6.4731e-04\n",
      "Epoch 186/200\n",
      " - 2s - loss: 5.7600e-04\n",
      "Epoch 187/200\n",
      " - 2s - loss: 6.0581e-04\n",
      "Epoch 188/200\n",
      " - 2s - loss: 6.0178e-04\n",
      "Epoch 189/200\n",
      " - 2s - loss: 6.3241e-04\n",
      "Epoch 190/200\n",
      " - 2s - loss: 6.0255e-04\n",
      "Epoch 191/200\n",
      " - 2s - loss: 6.2349e-04\n",
      "Epoch 192/200\n",
      " - 2s - loss: 6.2787e-04\n",
      "Epoch 193/200\n",
      " - 2s - loss: 6.2304e-04\n",
      "Epoch 194/200\n",
      " - 2s - loss: 5.8104e-04\n",
      "Epoch 195/200\n",
      " - 2s - loss: 5.8007e-04\n",
      "Epoch 196/200\n",
      " - 2s - loss: 6.0623e-04\n",
      "Epoch 197/200\n",
      " - 2s - loss: 6.2030e-04\n",
      "Epoch 198/200\n",
      " - 2s - loss: 6.0829e-04\n",
      "Epoch 199/200\n",
      " - 2s - loss: 5.7136e-04\n",
      "Epoch 200/200\n",
      " - 2s - loss: 6.0215e-04\n",
      "Train Score: 21.52 RMSE\n",
      "Test Score: 21.52 RMSE\n"
     ]
    }
   ],
   "source": [
    "look_back = 1\n",
    "trainX, trainY = create_dataset(train, look_back)\n",
    "testX, testY = create_dataset(test, look_back)\n",
    "trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
    "model = Sequential()\n",
    "model.add(LSTM(4, input_shape=(1, look_back),activation = 'tanh'))\n",
    "model.add(Dense(1,kernel_initializer = 'normal'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(.6))\n",
    "model.add(Dense(1,kernel_initializer = 'normal'))\n",
    "model.compile(loss='hinge', optimizer='Adagrad')\n",
    "model.fit(trainX, trainY, epochs=200, batch_size=256, verbose=2)\n",
    "trainPredict = model.predict(trainX)\n",
    "testPredict = model.predict(testX)\n",
    "trainPredict = scaler.inverse_transform(trainPredict)\n",
    "trainY = scaler.inverse_transform(trainY)\n",
    "testPredict = scaler.inverse_transform(testPredict)\n",
    "testY = scaler.inverse_transform(testY)\n",
    "trainScore = math.sqrt(mean_squared_error(trainY, trainPredict[:, 0]))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "testScore = math.sqrt(mean_squared_error(testY, testPredict[:, 0]))\n",
    "print('Test Score: %.2f RMSE' % (testScore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      " - 3s - loss: 0.8307\n",
      "Epoch 2/200\n",
      " - 2s - loss: 0.6101\n",
      "Epoch 3/200\n",
      " - 2s - loss: 0.4535\n",
      "Epoch 4/200\n",
      " - 2s - loss: 0.4196\n",
      "Epoch 5/200\n",
      " - 2s - loss: 0.4008\n",
      "Epoch 6/200\n",
      " - 2s - loss: 0.3811\n",
      "Epoch 7/200\n",
      " - 2s - loss: 0.3657\n",
      "Epoch 8/200\n",
      " - 2s - loss: 0.3513\n",
      "Epoch 9/200\n",
      " - 2s - loss: 0.3355\n",
      "Epoch 10/200\n",
      " - 2s - loss: 0.3215\n",
      "Epoch 11/200\n",
      " - 2s - loss: 0.3066\n",
      "Epoch 12/200\n",
      " - 2s - loss: 0.2953\n",
      "Epoch 13/200\n",
      " - 2s - loss: 0.2811\n",
      "Epoch 14/200\n",
      " - 2s - loss: 0.2678\n",
      "Epoch 15/200\n",
      " - 2s - loss: 0.2543\n",
      "Epoch 16/200\n",
      " - 2s - loss: 0.2429\n",
      "Epoch 17/200\n",
      " - 2s - loss: 0.2324\n",
      "Epoch 18/200\n",
      " - 2s - loss: 0.2206\n",
      "Epoch 19/200\n",
      " - 2s - loss: 0.2077\n",
      "Epoch 20/200\n",
      " - 2s - loss: 0.1977\n",
      "Epoch 21/200\n",
      " - 2s - loss: 0.1876\n",
      "Epoch 22/200\n",
      " - 2s - loss: 0.1771\n",
      "Epoch 23/200\n",
      " - 2s - loss: 0.1666\n",
      "Epoch 24/200\n",
      " - 2s - loss: 0.1570\n",
      "Epoch 25/200\n",
      " - 2s - loss: 0.1469\n",
      "Epoch 26/200\n",
      " - 2s - loss: 0.1370\n",
      "Epoch 27/200\n",
      " - 2s - loss: 0.1272\n",
      "Epoch 28/200\n",
      " - 2s - loss: 0.1180\n",
      "Epoch 29/200\n",
      " - 2s - loss: 0.1079\n",
      "Epoch 30/200\n",
      " - 2s - loss: 0.1000\n",
      "Epoch 31/200\n",
      " - 2s - loss: 0.0909\n",
      "Epoch 32/200\n",
      " - 2s - loss: 0.0817\n",
      "Epoch 33/200\n",
      " - 2s - loss: 0.0735\n",
      "Epoch 34/200\n",
      " - 2s - loss: 0.0647\n",
      "Epoch 35/200\n",
      " - 2s - loss: 0.0566\n",
      "Epoch 36/200\n",
      " - 2s - loss: 0.0482\n",
      "Epoch 37/200\n",
      " - 2s - loss: 0.0400\n",
      "Epoch 38/200\n",
      " - 2s - loss: 0.0322\n",
      "Epoch 39/200\n",
      " - 2s - loss: 0.0247\n",
      "Epoch 40/200\n",
      " - 2s - loss: 0.0176\n",
      "Epoch 41/200\n",
      " - 2s - loss: 0.0109\n",
      "Epoch 42/200\n",
      " - 2s - loss: 0.0061\n",
      "Epoch 43/200\n",
      " - 2s - loss: 0.0036\n",
      "Epoch 44/200\n",
      " - 2s - loss: 0.0027\n",
      "Epoch 45/200\n",
      " - 2s - loss: 0.0023\n",
      "Epoch 46/200\n",
      " - 2s - loss: 0.0021\n",
      "Epoch 47/200\n",
      " - 2s - loss: 0.0019\n",
      "Epoch 48/200\n",
      " - 2s - loss: 0.0018\n",
      "Epoch 49/200\n",
      " - 2s - loss: 0.0017\n",
      "Epoch 50/200\n",
      " - 2s - loss: 0.0016\n",
      "Epoch 51/200\n",
      " - 2s - loss: 0.0016\n",
      "Epoch 52/200\n",
      " - 2s - loss: 0.0015\n",
      "Epoch 53/200\n",
      " - 2s - loss: 0.0015\n",
      "Epoch 54/200\n",
      " - 2s - loss: 0.0014\n",
      "Epoch 55/200\n",
      " - 2s - loss: 0.0014\n",
      "Epoch 56/200\n",
      " - 2s - loss: 0.0014\n",
      "Epoch 57/200\n",
      " - 2s - loss: 0.0013\n",
      "Epoch 58/200\n",
      " - 2s - loss: 0.0013\n",
      "Epoch 59/200\n",
      " - 2s - loss: 0.0012\n",
      "Epoch 60/200\n",
      " - 2s - loss: 0.0013\n",
      "Epoch 61/200\n",
      " - 2s - loss: 0.0012\n",
      "Epoch 62/200\n",
      " - 2s - loss: 0.0012\n",
      "Epoch 63/200\n",
      " - 2s - loss: 0.0012\n",
      "Epoch 64/200\n",
      " - 2s - loss: 0.0012\n",
      "Epoch 65/200\n",
      " - 2s - loss: 0.0011\n",
      "Epoch 66/200\n",
      " - 2s - loss: 0.0011\n",
      "Epoch 67/200\n",
      " - 2s - loss: 0.0011\n",
      "Epoch 68/200\n",
      " - 2s - loss: 0.0011\n",
      "Epoch 69/200\n",
      " - 2s - loss: 0.0011\n",
      "Epoch 70/200\n",
      " - 2s - loss: 0.0011\n",
      "Epoch 71/200\n",
      " - 2s - loss: 0.0011\n",
      "Epoch 72/200\n",
      " - 2s - loss: 0.0011\n",
      "Epoch 73/200\n",
      " - 2s - loss: 0.0010\n",
      "Epoch 74/200\n",
      " - 2s - loss: 9.9442e-04\n",
      "Epoch 75/200\n",
      " - 2s - loss: 0.0010\n",
      "Epoch 76/200\n",
      " - 2s - loss: 0.0010\n",
      "Epoch 77/200\n",
      " - 2s - loss: 9.8665e-04\n",
      "Epoch 78/200\n",
      " - 2s - loss: 9.8651e-04\n",
      "Epoch 79/200\n",
      " - 2s - loss: 9.8182e-04\n",
      "Epoch 80/200\n",
      " - 2s - loss: 9.2517e-04\n",
      "Epoch 81/200\n",
      " - 2s - loss: 9.7558e-04\n",
      "Epoch 82/200\n",
      " - 2s - loss: 9.5371e-04\n",
      "Epoch 83/200\n",
      " - 2s - loss: 9.0679e-04\n",
      "Epoch 84/200\n",
      " - 2s - loss: 9.2187e-04\n",
      "Epoch 85/200\n",
      " - 2s - loss: 9.2387e-04\n",
      "Epoch 86/200\n",
      " - 2s - loss: 9.4076e-04\n",
      "Epoch 87/200\n",
      " - 2s - loss: 9.1462e-04\n",
      "Epoch 88/200\n",
      " - 2s - loss: 9.1318e-04\n",
      "Epoch 89/200\n",
      " - 2s - loss: 9.0664e-04\n",
      "Epoch 90/200\n",
      " - 2s - loss: 8.8470e-04\n",
      "Epoch 91/200\n",
      " - 2s - loss: 9.0590e-04\n",
      "Epoch 92/200\n",
      " - 2s - loss: 8.6065e-04\n",
      "Epoch 93/200\n",
      " - 2s - loss: 8.4307e-04\n",
      "Epoch 94/200\n",
      " - 2s - loss: 8.7234e-04\n",
      "Epoch 95/200\n",
      " - 2s - loss: 8.4228e-04\n",
      "Epoch 96/200\n",
      " - 2s - loss: 8.6356e-04\n",
      "Epoch 97/200\n",
      " - 2s - loss: 8.6688e-04\n",
      "Epoch 98/200\n",
      " - 2s - loss: 8.7035e-04\n",
      "Epoch 99/200\n",
      " - 2s - loss: 8.3551e-04\n",
      "Epoch 100/200\n",
      " - 2s - loss: 8.3903e-04\n",
      "Epoch 101/200\n",
      " - 2s - loss: 8.1913e-04\n",
      "Epoch 102/200\n",
      " - 2s - loss: 8.4773e-04\n",
      "Epoch 103/200\n",
      " - 2s - loss: 8.1687e-04\n",
      "Epoch 104/200\n",
      " - 2s - loss: 8.1098e-04\n",
      "Epoch 105/200\n",
      " - 2s - loss: 8.1130e-04\n",
      "Epoch 106/200\n",
      " - 2s - loss: 8.1095e-04\n",
      "Epoch 107/200\n",
      " - 2s - loss: 8.1459e-04\n",
      "Epoch 108/200\n",
      " - 2s - loss: 7.7372e-04\n",
      "Epoch 109/200\n",
      " - 2s - loss: 8.0760e-04\n",
      "Epoch 110/200\n",
      " - 2s - loss: 7.8652e-04\n",
      "Epoch 111/200\n",
      " - 2s - loss: 7.7059e-04\n",
      "Epoch 112/200\n",
      " - 2s - loss: 7.8755e-04\n",
      "Epoch 113/200\n",
      " - 2s - loss: 7.4802e-04\n",
      "Epoch 114/200\n",
      " - 2s - loss: 8.1557e-04\n",
      "Epoch 115/200\n",
      " - 2s - loss: 8.1264e-04\n",
      "Epoch 116/200\n",
      " - 2s - loss: 7.4216e-04\n",
      "Epoch 117/200\n",
      " - 2s - loss: 7.8558e-04\n",
      "Epoch 118/200\n",
      " - 2s - loss: 7.8138e-04\n",
      "Epoch 119/200\n",
      " - 2s - loss: 7.7366e-04\n",
      "Epoch 120/200\n",
      " - 2s - loss: 7.6225e-04\n",
      "Epoch 121/200\n",
      " - 2s - loss: 7.5668e-04\n",
      "Epoch 122/200\n",
      " - 2s - loss: 7.6196e-04\n",
      "Epoch 123/200\n",
      " - 2s - loss: 7.4826e-04\n",
      "Epoch 124/200\n",
      " - 2s - loss: 7.0882e-04\n",
      "Epoch 125/200\n",
      " - 2s - loss: 7.1410e-04\n",
      "Epoch 126/200\n",
      " - 2s - loss: 7.2136e-04\n",
      "Epoch 127/200\n",
      " - 2s - loss: 7.2160e-04\n",
      "Epoch 128/200\n",
      " - 2s - loss: 7.5520e-04\n",
      "Epoch 129/200\n",
      " - 2s - loss: 7.2272e-04\n",
      "Epoch 130/200\n",
      " - 2s - loss: 7.5375e-04\n",
      "Epoch 131/200\n",
      " - 2s - loss: 7.4472e-04\n",
      "Epoch 132/200\n",
      " - 2s - loss: 7.4642e-04\n",
      "Epoch 133/200\n",
      " - 2s - loss: 6.8692e-04\n",
      "Epoch 134/200\n",
      " - 2s - loss: 7.0809e-04\n",
      "Epoch 135/200\n",
      " - 2s - loss: 7.1969e-04\n",
      "Epoch 136/200\n",
      " - 2s - loss: 7.2971e-04\n",
      "Epoch 137/200\n",
      " - 2s - loss: 7.2883e-04\n",
      "Epoch 138/200\n",
      " - 2s - loss: 6.8255e-04\n",
      "Epoch 139/200\n",
      " - 2s - loss: 7.1830e-04\n",
      "Epoch 140/200\n",
      " - 2s - loss: 7.0349e-04\n",
      "Epoch 141/200\n",
      " - 2s - loss: 6.8406e-04\n",
      "Epoch 142/200\n",
      " - 2s - loss: 6.8475e-04\n",
      "Epoch 143/200\n",
      " - 2s - loss: 7.4207e-04\n",
      "Epoch 144/200\n",
      " - 2s - loss: 7.0185e-04\n",
      "Epoch 145/200\n",
      " - 2s - loss: 6.9994e-04\n",
      "Epoch 146/200\n",
      " - 2s - loss: 7.3973e-04\n",
      "Epoch 147/200\n",
      " - 2s - loss: 7.0444e-04\n",
      "Epoch 148/200\n",
      " - 2s - loss: 6.9611e-04\n",
      "Epoch 149/200\n",
      " - 2s - loss: 6.1990e-04\n",
      "Epoch 150/200\n",
      " - 2s - loss: 7.0972e-04\n",
      "Epoch 151/200\n",
      " - 2s - loss: 6.2952e-04\n",
      "Epoch 152/200\n",
      " - 2s - loss: 6.5846e-04\n",
      "Epoch 153/200\n",
      " - 2s - loss: 6.6699e-04\n",
      "Epoch 154/200\n",
      " - 2s - loss: 6.7146e-04\n",
      "Epoch 155/200\n",
      " - 2s - loss: 6.8244e-04\n",
      "Epoch 156/200\n",
      " - 2s - loss: 6.6330e-04\n",
      "Epoch 157/200\n",
      " - 2s - loss: 6.8350e-04\n",
      "Epoch 158/200\n",
      " - 2s - loss: 6.6061e-04\n",
      "Epoch 159/200\n",
      " - 2s - loss: 6.5987e-04\n",
      "Epoch 160/200\n",
      " - 2s - loss: 6.6627e-04\n",
      "Epoch 161/200\n",
      " - 2s - loss: 6.5062e-04\n",
      "Epoch 162/200\n",
      " - 2s - loss: 6.7100e-04\n",
      "Epoch 163/200\n",
      " - 2s - loss: 6.5854e-04\n",
      "Epoch 164/200\n",
      " - 2s - loss: 6.5702e-04\n",
      "Epoch 165/200\n",
      " - 2s - loss: 6.5205e-04\n",
      "Epoch 166/200\n",
      " - 2s - loss: 6.6763e-04\n",
      "Epoch 167/200\n",
      " - 2s - loss: 6.6023e-04\n",
      "Epoch 168/200\n",
      " - 2s - loss: 6.3189e-04\n",
      "Epoch 169/200\n",
      " - 2s - loss: 6.6434e-04\n",
      "Epoch 170/200\n",
      " - 2s - loss: 6.5327e-04\n",
      "Epoch 171/200\n",
      " - 2s - loss: 6.4023e-04\n",
      "Epoch 172/200\n",
      " - 2s - loss: 6.4688e-04\n",
      "Epoch 173/200\n",
      " - 2s - loss: 6.7292e-04\n",
      "Epoch 174/200\n",
      " - 2s - loss: 6.5603e-04\n",
      "Epoch 175/200\n",
      " - 2s - loss: 6.6379e-04\n",
      "Epoch 176/200\n",
      " - 2s - loss: 6.4075e-04\n",
      "Epoch 177/200\n",
      " - 2s - loss: 6.4264e-04\n",
      "Epoch 178/200\n",
      " - 2s - loss: 6.4634e-04\n",
      "Epoch 179/200\n",
      " - 2s - loss: 6.4427e-04\n",
      "Epoch 180/200\n",
      " - 2s - loss: 6.6160e-04\n",
      "Epoch 181/200\n",
      " - 2s - loss: 5.8417e-04\n",
      "Epoch 182/200\n",
      " - 2s - loss: 6.4985e-04\n",
      "Epoch 183/200\n",
      " - 2s - loss: 6.4581e-04\n",
      "Epoch 184/200\n",
      " - 2s - loss: 6.3642e-04\n",
      "Epoch 185/200\n",
      " - 2s - loss: 5.9953e-04\n",
      "Epoch 186/200\n",
      " - 2s - loss: 6.2595e-04\n",
      "Epoch 187/200\n",
      " - 2s - loss: 6.3828e-04\n",
      "Epoch 188/200\n",
      " - 2s - loss: 6.2694e-04\n",
      "Epoch 189/200\n",
      " - 2s - loss: 6.0084e-04\n",
      "Epoch 190/200\n",
      " - 2s - loss: 6.1325e-04\n",
      "Epoch 191/200\n",
      " - 2s - loss: 6.4785e-04\n",
      "Epoch 192/200\n",
      " - 2s - loss: 5.8592e-04\n",
      "Epoch 193/200\n",
      " - 2s - loss: 5.8117e-04\n",
      "Epoch 194/200\n",
      " - 2s - loss: 6.2251e-04\n",
      "Epoch 195/200\n",
      " - 2s - loss: 6.2629e-04\n",
      "Epoch 196/200\n",
      " - 2s - loss: 6.0175e-04\n",
      "Epoch 197/200\n",
      " - 2s - loss: 5.5230e-04\n",
      "Epoch 198/200\n",
      " - 2s - loss: 5.9497e-04\n",
      "Epoch 199/200\n",
      " - 2s - loss: 5.8189e-04\n",
      "Epoch 200/200\n",
      " - 2s - loss: 6.0936e-04\n",
      "Train Score: 21.51 RMSE\n",
      "Test Score: 21.50 RMSE\n"
     ]
    }
   ],
   "source": [
    "look_back = 1\n",
    "trainX, trainY = create_dataset(train, look_back)\n",
    "testX, testY = create_dataset(test, look_back)\n",
    "trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
    "model = Sequential()\n",
    "model.add(LSTM(4, input_shape=(1, look_back),activation = 'tanh'))\n",
    "model.add(Dense(1,kernel_initializer = 'random_uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(.6))\n",
    "model.add(Dense(1,kernel_initializer = 'random_uniform'))\n",
    "model.compile(loss='hinge', optimizer='Adagrad')\n",
    "model.fit(trainX, trainY, epochs=200, batch_size=256, verbose=2)\n",
    "trainPredict = model.predict(trainX)\n",
    "testPredict = model.predict(testX)\n",
    "trainPredict = scaler.inverse_transform(trainPredict)\n",
    "trainY = scaler.inverse_transform(trainY)\n",
    "testPredict = scaler.inverse_transform(testPredict)\n",
    "testY = scaler.inverse_transform(testY)\n",
    "trainScore = math.sqrt(mean_squared_error(trainY, trainPredict[:, 0]))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "testScore = math.sqrt(mean_squared_error(testY, testPredict[:, 0]))\n",
    "print('Test Score: %.2f RMSE' % (testScore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary:\n",
    "\n",
    "In this notebook, we have implemented LSTM recurrent neural networks for time series prediction of LogError = (log(Zestimate)-log(salesprice)) using 2016 property dataset and its corresponding log error values provided by zillow for home value prediction in Python using Keras and tensorflow deep learning libraries. \n",
    "\n",
    "Firstly, we have converted an array of values into a dataset matrix and fix random seed for reproducability. Normalized the dataset and split into training 90% and test dataset 10% later, a LSTM network was bulit with 4 inputs, 1 layer to predict the Error of existing Log Error and predicted Log Error by randonmly generated weights on gradient descent with various epochs, optimizers, activation and loss functions. With eposch=100, Activation functions= (sigmoid, relu, tanh), optimizers (adam, adagrad) and loss functions = mean_squared_error. Also, the weights are given manually for the gradient descent by kernel intializers which gives the best RSME of 0.16 on the train data and 0.15 on the test data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Referrence:\n",
    "https://www.sciencedirect.com/science/article/pii/S0377221703005484"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
