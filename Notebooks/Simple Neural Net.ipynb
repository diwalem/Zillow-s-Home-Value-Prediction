{
 "cells": [
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Neural Network\n",
    "\n",
    "Neural Networks are a machine learning framework that attempts to mimic the learning pattern of natural biological neural networks. neural networks receive inputs, then based on these inputs they produce an output signal to next. The process of creating a neural network begins with the most basic form, a single perceptron or Multi layer Perceptron.\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "It takes several input, processes it through multiple neurons from multiple hidden layers and returns the result using an output layer. This result estimation process is technically known as “Forward Propagation“.\n",
    "\n",
    "Next, we compare the result with actual output. The task is to make the output to neural network as close to actual (desired) output. Each of these neurons are contributing some error to final output. \n",
    "\n",
    "We try to minimize the value/ weight of neurons those are contributing more to the error and this happens while traveling back to the neurons of the neural network and finding where the error lies. This process is known as “Backward Propagation“. .\n",
    "\n",
    "In order to reduce these number of iterations to minimize the error, the neural networks use a common algorithm known as “Gradient Descent”, which helps to optimize the task quickly and efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reshma.nspr\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "C:\\Users\\Reshma.nspr\\Anaconda3\\lib\\site-packages\\statsmodels\\compat\\pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import numpy as numpy\n",
    "import pandas as pd\n",
    "import pylab\n",
    "import calendar\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "from sklearn import model_selection, preprocessing\n",
    "from scipy.stats import kendalltau\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout, BatchNormalization\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.optimizers import Adam\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras import callbacks\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import patsy\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.regression.quantile_regression import QuantReg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train, prop and sample data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reshma.nspr\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2785: DtypeWarning: Columns (22,32,34,49,55) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# Load train, Prop and sample\n",
    "print('Loading train, prop and sample data')\n",
    "train = pd.read_csv(\"train_2016_v2.csv\", parse_dates=[\"transactiondate\"])\n",
    "prop = pd.read_csv('properties_2016.csv')\n",
    "sample = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label Encoder\n",
    "LabelEncoder is a utility class to help normalize labels categorical values and to encode  such that they contain only values between 0 and n_classes-1. \n",
    "\n",
    "Here, we LabelEncode the properties dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Label Encoder on properties\n"
     ]
    }
   ],
   "source": [
    "print('Fitting Label Encoder on properties')\n",
    "for c in prop.columns:\n",
    "    prop[c]=prop[c].fillna(-1)\n",
    "    if prop[c].dtype == 'object':\n",
    "        lbl = LabelEncoder()\n",
    "        lbl.fit(list(prop[c].values))\n",
    "        prop[c] = lbl.transform(list(prop[c].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Create Test and Train sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating training set\n"
     ]
    }
   ],
   "source": [
    "#Create df_train and x_train y_train from that\n",
    "print('Creating training set')\n",
    "df_train = train.merge(prop, how='left', on='parcelid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating df_test\n"
     ]
    }
   ],
   "source": [
    "# Create df_test and test set\n",
    "print('Creating df_test')\n",
    "sample['parcelid'] = sample['ParcelId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge Sample with property data\n"
     ]
    }
   ],
   "source": [
    "print(\"Merge Sample with property data\")\n",
    "df_test = sample.merge(prop, on='parcelid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "INCLUDE_TIME_FEATURES = False\n",
    "INCLUDE_SEASONAL_FEATURES = True\n",
    "N_EPOCHS = 150\n",
    "BEST_EPOCH = False\n",
    "CV_ONLY = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Q4 Validation\n",
    "\n",
    "Here, we divide the whole year is didvided into four quarters (q1,q2,q3,q4) and the first three quaters are considered to be part of traning set and the validation set has the last qurater q4, Hence Q4 Validation.\n",
    "\n",
    "The quarters are divided based on the time and seasonal features. With the help of error in the Q4 validation set between it's existing Log Error and predicted Log Error it uses backpropagation in each Epoch to minimize it and build a better model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create x_train and y_train from df_train\n",
      "(81733, 55) (81733,)\n"
     ]
    }
   ],
   "source": [
    "df_train[\"transactiondate\"] = pd.to_datetime(df_train[\"transactiondate\"])\n",
    "df_train['transactiondate_quarter'] = df_train['transactiondate'].dt.quarter\n",
    "if INCLUDE_TIME_FEATURES:\n",
    "    df_train[\"transactiondate_year\"] = df_train[\"transactiondate\"].dt.year\n",
    "    df_train[\"transactiondate_month\"] = df_train[\"transactiondate\"].dt.month\n",
    "    df_train[\"transactiondate\"] = df_train[\"transactiondate\"].dt.day\n",
    "if INCLUDE_SEASONAL_FEATURES:\n",
    "    basedate = pd.to_datetime('2015-11-15').toordinal()\n",
    "    df_train['cos_season'] = \\\n",
    "        ( (pd.to_datetime(df_train['transactiondate']).apply(lambda x: x.toordinal()-basedate)) * \\\n",
    "          (2*np.pi/365.25) ).apply(np.cos)\n",
    "    df_train['sin_season'] = \\\n",
    "        ( (pd.to_datetime(df_train['transactiondate']).apply(lambda x: x.toordinal()-basedate)) * \\\n",
    "          (2*np.pi/365.25) ).apply(np.sin)\n",
    "\n",
    "    \n",
    "select_qtr4 = df_train[\"transactiondate_quarter\"] == 4\n",
    "\n",
    "print('Create x_train and y_train from df_train' )\n",
    "x_train_all = df_train.drop(['parcelid', 'logerror', 'transactiondate', 'propertyzoningdesc', 'propertycountylandusecode',\n",
    "                             'fireplacecnt', 'fireplaceflag'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not INCLUDE_TIME_FEATURES:\n",
    "     x_train_all = x_train_all.drop(['transactiondate_quarter'], axis=1)\n",
    "\n",
    "y_train_all = df_train[\"logerror\"]\n",
    "y_train = y_train_all[~select_qtr4]\n",
    "x_train = x_train_all[~select_qtr4]\n",
    "x_valid = x_train_all[select_qtr4]\n",
    "y_valid = y_train_all[select_qtr4]\n",
    "\n",
    "y_mean = np.mean(y_train)\n",
    "print(x_train.shape, y_train.shape)\n",
    "train_columns = x_train.columns\n",
    "\n",
    "for c in x_train.dtypes[x_train.dtypes == object].index.values:\n",
    "    x_train[c] = (x_train[c] == True)\n",
    "\n",
    "if INCLUDE_TIME_FEATURES:\n",
    "    df_test[\"transactiondate\"] = pd.to_datetime('2016-11-15')  # typical date for 2016 test data\n",
    "    df_test[\"transactiondate_year\"] = df_test[\"transactiondate\"].dt.year\n",
    "    df_test[\"transactiondate_month\"] = df_test[\"transactiondate\"].dt.month\n",
    "    df_test['transactiondate_quarter'] = df_test['transactiondate'].dt.quarter\n",
    "    df_test[\"transactiondate\"] = df_test[\"transactiondate\"].dt.day \n",
    "if INCLUDE_SEASONAL_FEATURES:\n",
    "    basedate = pd.to_datetime('2015-11-15').toordinal()\n",
    "    df_test['cos_season'] = np.cos( (pd.to_datetime('2016-11-15').toordinal() - basedate) * \\\n",
    "                                    (2*np.pi/365.25) )\n",
    "    df_test['sin_season'] = np.sin( (pd.to_datetime('2016-11-15').toordinal() - basedate) * \\\n",
    "                                    (2*np.pi/365.25) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_test: (2985217, 55)\n",
      "Preparing x_test:\n"
     ]
    }
   ],
   "source": [
    "x_test = df_test[train_columns]\n",
    "\n",
    "print('Shape of x_test:', x_test.shape)\n",
    "print(\"Preparing x_test:\")\n",
    "for c in x_test.dtypes[x_test.dtypes == object].index.values:\n",
    "    x_test[c] = (x_test[c] == True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Imputer \n",
    "Imputation transformer is used for completing missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "imputer= Imputer()\n",
    "imputer.fit(x_train.iloc[:, :])\n",
    "x_train = imputer.transform(x_train.iloc[:, :])\n",
    "imputer.fit(x_valid.iloc[:, :])\n",
    "x_valid = imputer.transform(x_valid.iloc[:, :])\n",
    "imputer.fit(x_test.iloc[:, :])\n",
    "x_test = imputer.transform(x_test.iloc[:, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Standard Scalar\n",
    "\n",
    "Standardizes features by removing the mean and scaling to unit variance\n",
    "\n",
    "Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Mean and standard deviation are then stored to be used on later data using the transform method.\n",
    "\n",
    "Standardization of a dataset is a common requirement for many machine learning estimators: they might behave badly if the individual feature do not more or less look like standard normally distributed data (e.g. Gaussian with 0 mean and unit variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.transform(x_test)\n",
    "x_valid = sc.fit_transform(x_valid)\n",
    "x_val = np.array(x_valid)\n",
    "y_val = np.array(y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Train Network\n",
    "Here we have five layered Neural network with inputs to the network \n",
    "starting from 400 to 1.\n",
    "###### Kernel Initializers\n",
    "Initializations define the way to set the initial random weights of Keras layers.\n",
    "##### Random Normal Initializer\n",
    "Initializer that generates tensors with a normal distribution.\n",
    "###### Batch Normalization\n",
    "We normalize the input layer by adjusting and scaling the activations and to speed up learning. In Batch normalization the output of a previous activation layer by subtracting the batch mean and dividing by the batch standard deviation.\n",
    "\n",
    "However, after this shift/scale of activation outputs by some randomly initialized parameters, the weights in the next layer are no longer optimal. GD (gradient descent) undoes this normalization if it’s a way for it to minimize the loss function.\n",
    "\n",
    "Here, we use mean absolute error loss function and Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len_x is: 55\n",
      "Train on 81733 samples, validate on 8542 samples\n",
      "Epoch 1/150\n",
      " - 30s - loss: 0.0760 - val_loss: 0.0666\n",
      "Epoch 2/150\n",
      " - 14s - loss: 0.0693 - val_loss: 0.0662\n",
      "Epoch 3/150\n",
      " - 14s - loss: 0.0686 - val_loss: 0.0655\n",
      "Epoch 4/150\n",
      " - 14s - loss: 0.0683 - val_loss: 0.0656\n",
      "Epoch 5/150\n",
      " - 14s - loss: 0.0682 - val_loss: 0.0657\n",
      "Epoch 6/150\n",
      " - 14s - loss: 0.0682 - val_loss: 0.0657\n",
      "Epoch 7/150\n",
      " - 14s - loss: 0.0681 - val_loss: 0.0654\n",
      "Epoch 8/150\n",
      " - 14s - loss: 0.0681 - val_loss: 0.0658\n",
      "Epoch 9/150\n",
      " - 13s - loss: 0.0681 - val_loss: 0.0654\n",
      "Epoch 10/150\n",
      " - 14s - loss: 0.0681 - val_loss: 0.0657\n",
      "Epoch 11/150\n",
      " - 14s - loss: 0.0680 - val_loss: 0.0662\n",
      "Epoch 12/150\n",
      " - 14s - loss: 0.0680 - val_loss: 0.0654\n",
      "Epoch 13/150\n",
      " - 13s - loss: 0.0680 - val_loss: 0.0654\n",
      "Epoch 14/150\n",
      " - 13s - loss: 0.0680 - val_loss: 0.0657\n",
      "Epoch 15/150\n",
      " - 13s - loss: 0.0679 - val_loss: 0.0658\n",
      "Epoch 16/150\n",
      " - 14s - loss: 0.0679 - val_loss: 0.0661\n",
      "Epoch 17/150\n",
      " - 14s - loss: 0.0679 - val_loss: 0.0665\n",
      "Epoch 18/150\n",
      " - 13s - loss: 0.0679 - val_loss: 0.0654\n",
      "Epoch 19/150\n",
      " - 13s - loss: 0.0679 - val_loss: 0.0651\n",
      "Epoch 20/150\n",
      " - 13s - loss: 0.0679 - val_loss: 0.0655\n",
      "Epoch 21/150\n",
      " - 13s - loss: 0.0679 - val_loss: 0.0655\n",
      "Epoch 22/150\n",
      " - 14s - loss: 0.0678 - val_loss: 0.0653\n",
      "Epoch 23/150\n",
      " - 14s - loss: 0.0678 - val_loss: 0.0656\n",
      "Epoch 24/150\n",
      " - 14s - loss: 0.0679 - val_loss: 0.0652\n",
      "Epoch 25/150\n",
      " - 15s - loss: 0.0678 - val_loss: 0.0653\n",
      "Epoch 26/150\n",
      " - 18s - loss: 0.0678 - val_loss: 0.0657\n",
      "Epoch 27/150\n",
      " - 18s - loss: 0.0678 - val_loss: 0.0654\n",
      "Epoch 28/150\n",
      " - 18s - loss: 0.0678 - val_loss: 0.0654\n",
      "Epoch 29/150\n",
      " - 18s - loss: 0.0678 - val_loss: 0.0657\n",
      "Epoch 30/150\n",
      " - 18s - loss: 0.0678 - val_loss: 0.0656\n",
      "Epoch 31/150\n",
      " - 18s - loss: 0.0678 - val_loss: 0.0661\n",
      "Epoch 32/150\n",
      " - 18s - loss: 0.0678 - val_loss: 0.0654\n",
      "Epoch 33/150\n",
      " - 19s - loss: 0.0678 - val_loss: 0.0654\n",
      "Epoch 34/150\n",
      " - 19s - loss: 0.0678 - val_loss: 0.0651\n",
      "Epoch 35/150\n",
      " - 18s - loss: 0.0677 - val_loss: 0.0657\n",
      "Epoch 36/150\n",
      " - 18s - loss: 0.0678 - val_loss: 0.0653\n",
      "Epoch 37/150\n",
      " - 18s - loss: 0.0678 - val_loss: 0.0655\n",
      "Epoch 38/150\n",
      " - 18s - loss: 0.0677 - val_loss: 0.0656\n",
      "Epoch 39/150\n",
      " - 18s - loss: 0.0677 - val_loss: 0.0652\n",
      "Epoch 40/150\n",
      " - 18s - loss: 0.0677 - val_loss: 0.0654\n",
      "Epoch 41/150\n",
      " - 19s - loss: 0.0677 - val_loss: 0.0655\n",
      "Epoch 42/150\n",
      " - 18s - loss: 0.0678 - val_loss: 0.0656\n",
      "Epoch 43/150\n",
      " - 18s - loss: 0.0677 - val_loss: 0.0653\n",
      "Epoch 44/150\n",
      " - 18s - loss: 0.0677 - val_loss: 0.0654\n",
      "Epoch 45/150\n",
      " - 18s - loss: 0.0677 - val_loss: 0.0655\n",
      "Epoch 46/150\n",
      " - 18s - loss: 0.0677 - val_loss: 0.0663\n",
      "Epoch 47/150\n",
      " - 18s - loss: 0.0677 - val_loss: 0.0652\n",
      "Epoch 48/150\n",
      " - 19s - loss: 0.0677 - val_loss: 0.0653\n",
      "Epoch 49/150\n",
      " - 19s - loss: 0.0677 - val_loss: 0.0653\n",
      "Epoch 50/150\n",
      " - 19s - loss: 0.0677 - val_loss: 0.0653\n",
      "Epoch 51/150\n",
      " - 19s - loss: 0.0677 - val_loss: 0.0657\n",
      "Epoch 52/150\n",
      " - 19s - loss: 0.0677 - val_loss: 0.0653\n",
      "Epoch 53/150\n",
      " - 19s - loss: 0.0677 - val_loss: 0.0652\n",
      "Epoch 54/150\n",
      " - 18s - loss: 0.0677 - val_loss: 0.0655\n",
      "Epoch 55/150\n",
      " - 18s - loss: 0.0676 - val_loss: 0.0651\n",
      "Epoch 56/150\n",
      " - 18s - loss: 0.0677 - val_loss: 0.0659\n",
      "Epoch 57/150\n",
      " - 19s - loss: 0.0677 - val_loss: 0.0656\n",
      "Epoch 58/150\n",
      " - 18s - loss: 0.0676 - val_loss: 0.0653\n",
      "Epoch 59/150\n",
      " - 18s - loss: 0.0676 - val_loss: 0.0652\n",
      "Epoch 60/150\n",
      " - 18s - loss: 0.0676 - val_loss: 0.0654\n",
      "Epoch 61/150\n",
      " - 18s - loss: 0.0676 - val_loss: 0.0653\n",
      "Epoch 62/150\n",
      " - 19s - loss: 0.0676 - val_loss: 0.0658\n",
      "Epoch 63/150\n",
      " - 18s - loss: 0.0676 - val_loss: 0.0658\n",
      "Epoch 64/150\n",
      " - 18s - loss: 0.0676 - val_loss: 0.0657\n",
      "Epoch 65/150\n",
      " - 18s - loss: 0.0676 - val_loss: 0.0656\n",
      "Epoch 66/150\n",
      " - 18s - loss: 0.0676 - val_loss: 0.0654\n",
      "Epoch 67/150\n",
      " - 18s - loss: 0.0676 - val_loss: 0.0658\n",
      "Epoch 68/150\n",
      " - 18s - loss: 0.0676 - val_loss: 0.0654\n",
      "Epoch 69/150\n",
      " - 18s - loss: 0.0676 - val_loss: 0.0652\n",
      "Epoch 70/150\n",
      " - 18s - loss: 0.0676 - val_loss: 0.0651\n",
      "Epoch 71/150\n",
      " - 18s - loss: 0.0676 - val_loss: 0.0653\n",
      "Epoch 72/150\n",
      " - 18s - loss: 0.0676 - val_loss: 0.0654\n",
      "Epoch 73/150\n",
      " - 18s - loss: 0.0676 - val_loss: 0.0660\n",
      "Epoch 74/150\n",
      " - 18s - loss: 0.0676 - val_loss: 0.0653\n",
      "Epoch 75/150\n",
      " - 18s - loss: 0.0676 - val_loss: 0.0654\n",
      "Epoch 76/150\n",
      " - 18s - loss: 0.0676 - val_loss: 0.0653\n",
      "Epoch 77/150\n",
      " - 20s - loss: 0.0676 - val_loss: 0.0653\n",
      "Epoch 78/150\n",
      " - 20s - loss: 0.0676 - val_loss: 0.0654\n",
      "Epoch 79/150\n",
      " - 20s - loss: 0.0676 - val_loss: 0.0652\n",
      "Epoch 80/150\n",
      " - 24s - loss: 0.0676 - val_loss: 0.0660\n",
      "Epoch 81/150\n",
      " - 18s - loss: 0.0676 - val_loss: 0.0652\n",
      "Epoch 82/150\n",
      " - 18s - loss: 0.0676 - val_loss: 0.0652\n",
      "Epoch 83/150\n",
      " - 18s - loss: 0.0676 - val_loss: 0.0659\n",
      "Epoch 84/150\n",
      " - 18s - loss: 0.0676 - val_loss: 0.0652\n",
      "Epoch 85/150\n",
      " - 18s - loss: 0.0676 - val_loss: 0.0652\n",
      "Epoch 86/150\n",
      " - 18s - loss: 0.0676 - val_loss: 0.0655\n",
      "Epoch 87/150\n",
      " - 18s - loss: 0.0676 - val_loss: 0.0652\n",
      "Epoch 88/150\n",
      " - 18s - loss: 0.0675 - val_loss: 0.0653\n",
      "Epoch 89/150\n",
      " - 18s - loss: 0.0675 - val_loss: 0.0656\n",
      "Epoch 90/150\n",
      " - 18s - loss: 0.0675 - val_loss: 0.0654\n",
      "Epoch 91/150\n",
      " - 18s - loss: 0.0676 - val_loss: 0.0660\n",
      "Epoch 92/150\n",
      " - 18s - loss: 0.0676 - val_loss: 0.0651\n",
      "Epoch 93/150\n",
      " - 18s - loss: 0.0675 - val_loss: 0.0654\n",
      "Epoch 94/150\n",
      " - 18s - loss: 0.0675 - val_loss: 0.0659\n",
      "Epoch 95/150\n",
      " - 19s - loss: 0.0675 - val_loss: 0.0656\n",
      "Epoch 96/150\n",
      " - 18s - loss: 0.0675 - val_loss: 0.0661\n",
      "Epoch 97/150\n",
      " - 18s - loss: 0.0675 - val_loss: 0.0654\n",
      "Epoch 98/150\n",
      " - 21s - loss: 0.0675 - val_loss: 0.0653\n",
      "Epoch 99/150\n",
      " - 19s - loss: 0.0675 - val_loss: 0.0653\n",
      "Epoch 100/150\n",
      " - 18s - loss: 0.0675 - val_loss: 0.0656\n",
      "Epoch 101/150\n",
      " - 18s - loss: 0.0675 - val_loss: 0.0653\n",
      "Epoch 102/150\n",
      " - 18s - loss: 0.0675 - val_loss: 0.0658\n",
      "Epoch 103/150\n",
      " - 18s - loss: 0.0675 - val_loss: 0.0653\n",
      "Epoch 104/150\n",
      " - 18s - loss: 0.0675 - val_loss: 0.0653\n",
      "Epoch 105/150\n",
      " - 18s - loss: 0.0675 - val_loss: 0.0660\n",
      "Epoch 106/150\n",
      " - 18s - loss: 0.0675 - val_loss: 0.0652\n",
      "Epoch 107/150\n",
      " - 18s - loss: 0.0675 - val_loss: 0.0653\n",
      "Epoch 108/150\n",
      " - 18s - loss: 0.0675 - val_loss: 0.0651\n",
      "Epoch 109/150\n",
      " - 18s - loss: 0.0675 - val_loss: 0.0654\n",
      "Epoch 110/150\n",
      " - 18s - loss: 0.0675 - val_loss: 0.0652\n",
      "Epoch 111/150\n",
      " - 18s - loss: 0.0675 - val_loss: 0.0652\n",
      "Epoch 112/150\n",
      " - 16s - loss: 0.0675 - val_loss: 0.0656\n",
      "Epoch 113/150\n",
      " - 15s - loss: 0.0675 - val_loss: 0.0655\n",
      "Epoch 114/150\n",
      " - 17s - loss: 0.0675 - val_loss: 0.0655\n",
      "Epoch 115/150\n",
      " - 18s - loss: 0.0675 - val_loss: 0.0653\n",
      "Epoch 116/150\n",
      " - 16s - loss: 0.0674 - val_loss: 0.0652\n",
      "Epoch 117/150\n",
      " - 16s - loss: 0.0675 - val_loss: 0.0655\n",
      "Epoch 118/150\n",
      " - 19s - loss: 0.0675 - val_loss: 0.0655\n",
      "Epoch 119/150\n",
      " - 19s - loss: 0.0675 - val_loss: 0.0655\n",
      "Epoch 120/150\n",
      " - 19s - loss: 0.0675 - val_loss: 0.0652\n",
      "Epoch 121/150\n",
      " - 19s - loss: 0.0674 - val_loss: 0.0654\n",
      "Epoch 122/150\n",
      " - 19s - loss: 0.0675 - val_loss: 0.0652\n",
      "Epoch 123/150\n",
      " - 20s - loss: 0.0674 - val_loss: 0.0653\n",
      "Epoch 124/150\n",
      " - 18s - loss: 0.0675 - val_loss: 0.0654\n",
      "Epoch 125/150\n",
      " - 19s - loss: 0.0674 - val_loss: 0.0654\n",
      "Epoch 126/150\n",
      " - 18s - loss: 0.0674 - val_loss: 0.0655\n",
      "Epoch 127/150\n",
      " - 20s - loss: 0.0674 - val_loss: 0.0655\n",
      "Epoch 128/150\n",
      " - 17s - loss: 0.0674 - val_loss: 0.0652\n",
      "Epoch 129/150\n",
      " - 17s - loss: 0.0675 - val_loss: 0.0653\n",
      "Epoch 130/150\n",
      " - 19s - loss: 0.0675 - val_loss: 0.0653\n",
      "Epoch 131/150\n",
      " - 18s - loss: 0.0675 - val_loss: 0.0651\n",
      "Epoch 132/150\n",
      " - 19s - loss: 0.0674 - val_loss: 0.0655\n",
      "Epoch 133/150\n",
      " - 21s - loss: 0.0674 - val_loss: 0.0657\n",
      "Epoch 134/150\n",
      " - 20s - loss: 0.0674 - val_loss: 0.0652\n",
      "Epoch 135/150\n",
      " - 16s - loss: 0.0674 - val_loss: 0.0657\n",
      "Epoch 136/150\n",
      " - 16s - loss: 0.0674 - val_loss: 0.0656\n",
      "Epoch 137/150\n",
      " - 16s - loss: 0.0674 - val_loss: 0.0655\n",
      "Epoch 138/150\n",
      " - 16s - loss: 0.0674 - val_loss: 0.0657\n",
      "Epoch 139/150\n",
      " - 16s - loss: 0.0674 - val_loss: 0.0659\n",
      "Epoch 140/150\n",
      " - 16s - loss: 0.0674 - val_loss: 0.0656\n",
      "Epoch 141/150\n",
      " - 16s - loss: 0.0674 - val_loss: 0.0659\n",
      "Epoch 142/150\n",
      " - 16s - loss: 0.0674 - val_loss: 0.0657\n",
      "Epoch 143/150\n",
      " - 16s - loss: 0.0674 - val_loss: 0.0651\n",
      "Epoch 144/150\n",
      " - 17s - loss: 0.0674 - val_loss: 0.0656\n",
      "Epoch 145/150\n",
      " - 17s - loss: 0.0674 - val_loss: 0.0651\n",
      "Epoch 146/150\n",
      " - 17s - loss: 0.0674 - val_loss: 0.0652\n",
      "Epoch 147/150\n",
      " - 17s - loss: 0.0674 - val_loss: 0.0653\n",
      "Epoch 148/150\n",
      " - 17s - loss: 0.0674 - val_loss: 0.0652\n",
      "Epoch 149/150\n",
      " - 17s - loss: 0.0674 - val_loss: 0.0655\n",
      "Epoch 150/150\n",
      " - 17s - loss: 0.0674 - val_loss: 0.0657\n"
     ]
    }
   ],
   "source": [
    "len_x=int(x_train.shape[1])\n",
    "print(\"len_x is:\",len_x)\n",
    "\n",
    "nn = Sequential()\n",
    "nn.add(Dense(units = 400 , kernel_initializer = 'normal', input_dim = len_x))\n",
    "nn.add(PReLU())\n",
    "nn.add(Dropout(.4))\n",
    "nn.add(Dense(units = 160 , kernel_initializer = 'normal'))\n",
    "nn.add(PReLU())\n",
    "nn.add(BatchNormalization())\n",
    "nn.add(Dropout(.6))\n",
    "nn.add(Dense(units = 64 , kernel_initializer = 'normal'))\n",
    "nn.add(PReLU())\n",
    "nn.add(BatchNormalization())\n",
    "nn.add(Dropout(.5))\n",
    "nn.add(Dense(units = 26, kernel_initializer = 'normal'))\n",
    "nn.add(PReLU())\n",
    "nn.add(BatchNormalization())\n",
    "nn.add(Dropout(.6))\n",
    "nn.add(Dense(1, kernel_initializer='normal'))\n",
    "\n",
    "nn.compile(loss='mae', optimizer='Adam')\n",
    "\n",
    "nn.fit(np.array(x_train), np.array(y_train), batch_size = 32, epochs = N_EPOCHS, verbose=2, \n",
    "       validation_data=(x_val,y_val))\n",
    "\n",
    "valid_pred = nn.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 0.148\n"
     ]
    }
   ],
   "source": [
    "rmse = np.sqrt(mean_squared_error(y_val,valid_pred))\n",
    "print('Test RMSE: %.3f' % rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Predict and Write Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating predictions...\n",
      "\n",
      "Preparing results for write...\n",
      "\n",
      "Writing results to disk:\n",
      "\n",
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if not CV_ONLY:\n",
    "    print(\"\\nGenerating predictions...\")\n",
    "    y_pred_ann = nn.predict(x_test)\n",
    "\n",
    "    print( \"\\nPreparing results for write...\" )\n",
    "    y_pred = y_pred_ann.flatten()\n",
    "\n",
    "    output = pd.DataFrame({'ParcelId': prop['parcelid'].astype(np.int32),\n",
    "            '201610': y_pred, '201611': y_pred, '201612': y_pred,\n",
    "            '201710': y_pred, '201711': y_pred, '201712': y_pred})\n",
    "    # set col 'ParceID' to first col\n",
    "    cols = output.columns.tolist()\n",
    "    cols = cols[-1:] + cols[:-1]\n",
    "    output = output[cols]\n",
    "\n",
    "    print( \"\\nWriting results to disk:\" )\n",
    "    output.to_csv('Only_ANN_{}.csv'.format(datetime.now().strftime('%Y%m%d_%H%M%S')), index=False)\n",
    "\n",
    "\n",
    "\n",
    "print( \"\\nFinished!\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary:\n",
    "\n",
    "In this notebook, we have implemented Simple neural networks with 5 layers for  prediction of LogError = (log(Zestimate)-log(salesprice)) using 2016 property dataset and its corresponding log error values provided by zillow for home value prediction in Python using Keras and tensorflow deep learning libraries.\n",
    "\n",
    "Original dataset is prepared accordingly and required features are label encoded rest of the features which are almost zero or few are dropped. Later,the whole year is didvided into four quarters and the first three quaters are considered to be part of traning set and the validation set has the last qurater q4, Hence Q4 Validation.\n",
    "\n",
    "The quarters are divided based on the time and seasonal features. With the help of error in the Q4 validation set between it's existing Log Error and predicted Log Error it uses backpropagation in each Epoch to minimize it and build a better model.\n",
    "\n",
    "Finally, we have predicted logerror values of 2016 and 2017 for the last quarter (from November to December) in the test dataset and the results are written to a csv file.\n",
    "Calculate RMSE for the Network built can be seen as 0.148 which means that there is minimal error in the logerror gives us the better predictions.Further, the model can improvised by add more layers or changing the backpropagation parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Referrence:\n",
    "\n",
    "https://www.sciencedirect.com/science/article/pii/S0377221703005484\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
